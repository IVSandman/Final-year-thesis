{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cde758-b468-4412-aa1b-faffc4b56fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old model\n",
    "\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Custom Dataset for k-mer frequency data\n",
    "class KmerFrequencyDataset(Dataset):\n",
    "    def __init__(self, kmer_score_file):\n",
    "        # Load scoring file\n",
    "        self.data = pd.read_csv(kmer_score_file)\n",
    "\n",
    "        # Extract features and labels\n",
    "        self.features = self.data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "        self.labels = self.data['status'].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Define LSTM model with an embedding layer\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True, dropout=0.5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim2, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension for LSTM\n",
    "        \n",
    "        # Pass through LSTM layers\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        \n",
    "        # Extract the last output of the LSTM\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)  # Output probabilities\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 128  # New hyperparameter for embedding layer\n",
    "hidden_dim1 = 64\n",
    "hidden_dim2 = 32\n",
    "dense_units = 64\n",
    "num_classes = 3\n",
    "learning_rate = 0.005\n",
    "num_epochs = 180\n",
    "batch_size = 32\n",
    "\n",
    "# File paths\n",
    "kmer_score_file = '/home/user/torch_shrimp/until-tools/mod/k-mer/test2_new.csv'\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = KmerFrequencyDataset(kmer_score_file)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "input_dim = dataset.features.shape[1]  # Automatically adjust to input feature size\n",
    "model = LSTMModel(input_dim, embedding_dim, hidden_dim1, hidden_dim2, dense_units, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += y_batch.size(0)\n",
    "            val_correct += (predicted == y_batch).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        test_total += y_batch.size(0)\n",
    "        test_correct += (predicted == y_batch).sum().item()\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'new_1.pth')\n",
    "print(\"Model training completed and saved.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d45d7-8789-4de9-b52a-b83db94e8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New model (NO log)\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Custom Dataset for k-mer frequency data\n",
    "class KmerFrequencyDataset(Dataset):\n",
    "    def __init__(self, kmer_score_file):\n",
    "        # Load scoring file\n",
    "        self.data = pd.read_csv(kmer_score_file)\n",
    "\n",
    "        # Extract features and labels\n",
    "        self.features = self.data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "        self.labels = self.data['status'].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Define LSTM model with embedding layer\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True, dropout=0.5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim2, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_dim) -> (batch_size, embedding_dim)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension for LSTM\n",
    "\n",
    "        # LSTM layers\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "\n",
    "        # Extract the last output of the LSTM\n",
    "        x = x[:, -1, :]  # Use the last hidden state\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)  # Output probabilities\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 128\n",
    "hidden_dim1 = 64\n",
    "hidden_dim2 = 32\n",
    "dense_units = 64\n",
    "num_classes = 3\n",
    "learning_rate = 0.005\n",
    "num_epochs = 180\n",
    "batch_size = 32\n",
    "\n",
    "# File paths\n",
    "kmer_score_file = '/home/user/torch_shrimp/until-tools/mod/k-mer/test2_new.csv'\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = KmerFrequencyDataset(kmer_score_file)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "input_dim = dataset.features.shape[1]  # Automatically adjust to input feature size\n",
    "model = LSTMModel(input_dim, embedding_dim, hidden_dim1, hidden_dim2, dense_units, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += y_batch.size(0)\n",
    "            val_correct += (predicted == y_batch).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(torch.float32)\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "        test_total += y_batch.size(0)\n",
    "        test_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'updated_model_1.pth')\n",
    "print(\"Model training completed and saved.\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c276b4c-822e-4adf-8e3c-f94d7f5a0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training model version 1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "log_file = f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# Function to log hyperparameters\n",
    "def log_hyperparameters(hyperparams):\n",
    "    logging.info(\"Hyperparameters:\")\n",
    "    logging.info(json.dumps(hyperparams, indent=4))\n",
    "\n",
    "# Custom Dataset for k-mer frequency data\n",
    "class KmerFrequencyDataset(Dataset):\n",
    "    def __init__(self, kmer_score_file):\n",
    "        self.data = pd.read_csv(kmer_score_file)\n",
    "        self.features = self.data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "        self.labels = self.data['status'].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True, dropout=0.5)\n",
    "        self.fc1 = nn.Linear(hidden_dim2, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"embedding_dim\": 128,\n",
    "    \"hidden_dim1\": 64,\n",
    "    \"hidden_dim2\": 32,\n",
    "    \"dense_units\": 64,\n",
    "    \"num_classes\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"num_epochs\": 600,\n",
    "    \"batch_size\": 32,\n",
    "    \"file_path\": '/home/user/torch_shrimp/until-tools/mod/k-mer/train400.csv'\n",
    "}\n",
    "log_hyperparameters(hyperparams)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = KmerFrequencyDataset(hyperparams[\"file_path\"])\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparams[\"batch_size\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams[\"batch_size\"])\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "input_dim = dataset.features.shape[1]\n",
    "#model = LSTMModel(input_dim, **hyperparams)\n",
    "model = LSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "    hidden_dim1=hyperparams[\"hidden_dim1\"],\n",
    "    hidden_dim2=hyperparams[\"hidden_dim2\"],\n",
    "    dense_units=hyperparams[\"dense_units\"],\n",
    "    num_classes=hyperparams[\"num_classes\"]\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(hyperparams[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += y_batch.size(0)\n",
    "            val_correct += (predicted == y_batch).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    logging.info(f\"Epoch [{epoch + 1}/{hyperparams['num_epochs']}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(torch.float32)\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "        test_total += y_batch.size(0)\n",
    "        test_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "logging.info(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "logging.info(\"Classification Report:\")\n",
    "logging.info(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Save the model\n",
    "model_save_path = \"file_tune_5.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "logging.info(f\"Model training completed and saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecfd143-0305-42ca-85a4-fe16528e9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "log_file = f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# Function to log hyperparameters\n",
    "def log_hyperparameters(hyperparams):\n",
    "    logging.info(\"Hyperparameters:\")\n",
    "    logging.info(json.dumps(hyperparams, indent=4))\n",
    "\n",
    "# Custom Dataset for k-mer frequency data\n",
    "class KmerFrequencyDataset(Dataset):\n",
    "    def __init__(self, kmer_score_file):\n",
    "        self.data = pd.read_csv(kmer_score_file)\n",
    "        self.features = self.data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "        self.labels = self.data['status'].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True, dropout=0.5)\n",
    "        self.fc1 = nn.Linear(hidden_dim2, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"embedding_dim\": 128,\n",
    "    \"hidden_dim1\": 64,\n",
    "    \"hidden_dim2\": 32,\n",
    "    \"dense_units\": 64,\n",
    "    \"num_classes\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"num_epochs\": 500,\n",
    "    \"batch_size\": 32,\n",
    "    \"train_size\" : 0.8,\n",
    "    \"val_size\" : 0.1,\n",
    "    \"file_path\": '/home/user/torch_shrimp/until-tools/mod/k-mer/dataset/train400.csv'\n",
    "}\n",
    "log_hyperparameters(hyperparams)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = KmerFrequencyDataset(hyperparams[\"file_path\"])\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(hyperparams[\"train_size\"] * dataset_size)\n",
    "val_size = int(hyperparams[\"val_size\"] * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparams[\"batch_size\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams[\"batch_size\"])\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "input_dim = dataset.features.shape[1]\n",
    "#model = LSTMModel(input_dim, **hyperparams)\n",
    "model = LSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "    hidden_dim1=hyperparams[\"hidden_dim1\"],\n",
    "    hidden_dim2=hyperparams[\"hidden_dim2\"],\n",
    "    dense_units=hyperparams[\"dense_units\"],\n",
    "    num_classes=hyperparams[\"num_classes\"]\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(hyperparams[\"num_epochs\"]):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += y_batch.size(0)\n",
    "            val_correct += (predicted == y_batch).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    # Test phase (add test computation inside the loop)\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(torch.float32)\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            test_total += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "    # Log all results in one place\n",
    "    logging.info(f\"Epoch [{epoch + 1}/{hyperparams['num_epochs']}] - \"\n",
    "                 f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                 f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%, \"\n",
    "                 f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(torch.float32)\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "        test_total += y_batch.size(0)\n",
    "        test_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "logging.info(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "logging.info(\"Classification Report:\")\n",
    "logging.info(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Save the models\n",
    "model_save_path = \"model_9_v2.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "logging.info(f\"Model training completed and saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d129b39-b248-442b-8dda-e34d191a945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set log directory\n",
    "log_dir = \"/home/user/torch_shrimp/until-tools/mod/k-mer/es/batch1\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Ensure the log filename is the same as the directory name\n",
    "log_file = os.path.join(log_dir, f\"{os.path.basename(log_dir)}.txt\")\n",
    "\n",
    "# Remove previous logging handlers to prevent duplicate logs\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# Function to log hyperparameters\n",
    "def log_hyperparameters(hyperparams):\n",
    "    logging.info(\"Hyperparameters:\")\n",
    "    logging.info(json.dumps(hyperparams, indent=4))\n",
    "\n",
    "# Custom Dataset for k-mer frequency data\n",
    "class KmerFrequencyDataset(Dataset):\n",
    "    def __init__(self, kmer_score_file):\n",
    "        self.data = pd.read_csv(kmer_score_file)\n",
    "        self.features = self.data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "        self.labels = self.data['status'].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True, dropout=0.5)\n",
    "        self.fc1 = nn.Linear(hidden_dim2, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"embedding_dim\": 128,\n",
    "    \"hidden_dim1\": 64,\n",
    "    \"hidden_dim2\": 32,\n",
    "    \"dense_units\": 64,\n",
    "    \"num_classes\": 3,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_epochs\": 500,\n",
    "    \"batch_size\": 32,\n",
    "    \"train_size\": 0.8,\n",
    "    \"val_size\": 0.1,\n",
    "    \"file_path\": '/home/user/torch_shrimp/until-tools/mod/k-mer/dataset/train400.csv',\n",
    "    \"patience\": 50\n",
    "}\n",
    "log_hyperparameters(hyperparams)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = KmerFrequencyDataset(hyperparams[\"file_path\"])\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(hyperparams[\"train_size\"] * dataset_size)\n",
    "val_size = int(hyperparams[\"val_size\"] * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparams[\"batch_size\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams[\"batch_size\"])\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "input_dim = dataset.features.shape[1]\n",
    "model = LSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "    hidden_dim1=hyperparams[\"hidden_dim1\"],\n",
    "    hidden_dim2=hyperparams[\"hidden_dim2\"],\n",
    "    dense_units=hyperparams[\"dense_units\"],\n",
    "    num_classes=hyperparams[\"num_classes\"]\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "\n",
    "# Training and validation loop\n",
    "best_val_loss = float('inf')\n",
    "patience = hyperparams[\"patience\"]\n",
    "patience_counter = 0\n",
    "best_model_path = os.path.join(log_dir, \"best_model.pth\")\n",
    "final_model_path = os.path.join(log_dir, \"final_model.pth\")\n",
    "\n",
    "for epoch in range(hyperparams[\"num_epochs\"]):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += y_batch.size(0)\n",
    "            val_correct += (predicted == y_batch).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    logging.info(f\"Epoch [{epoch + 1}/{hyperparams['num_epochs']}] - \"\n",
    "                 f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                 f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            logging.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "logging.info(f\"Training completed. Best model saved at {best_model_path}, Final model saved at {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c6f1f-e8b5-4423-98ad-16bc7916ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 4 (missing test)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set log directory\n",
    "log_dir = \"/home/user/torch_shrimp/until-tools/mod/k-mer/es/batch0\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Ensure the log filename is the same as the directory name\n",
    "log_file = os.path.join(log_dir, f\"{os.path.basename(log_dir)}.txt\")\n",
    "\n",
    "# Remove previous logging handlers to prevent duplicate logs\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# Function to log hyperparameters\n",
    "def log_hyperparameters(hyperparams):\n",
    "    logging.info(\"Hyperparameters:\")\n",
    "    logging.info(json.dumps(hyperparams, indent=4))\n",
    "\n",
    "# Custom Dataset for k-mer frequency data\n",
    "class KmerFrequencyDataset(Dataset):\n",
    "    def __init__(self, kmer_score_file):\n",
    "        self.data = pd.read_csv(kmer_score_file)\n",
    "        self.features = self.data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "        self.labels = self.data['status'].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True, dropout=0.5)\n",
    "        self.fc1 = nn.Linear(hidden_dim2, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"embedding_dim\": 128,\n",
    "    \"hidden_dim1\": 64,\n",
    "    \"hidden_dim2\": 32,\n",
    "    \"dense_units\": 64,\n",
    "    \"num_classes\": 3,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_epochs\": 500,\n",
    "    \"batch_size\": 32,\n",
    "    \"train_size\": 0.7,\n",
    "    \"val_size\": 0.1,\n",
    "    \"file_path\": \"/home/user/torch_shrimp/until-tools/mod/k-mer/dataset/train200.csv\",\n",
    "    \"patience\": 50\n",
    "}\n",
    "log_hyperparameters(hyperparams)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = KmerFrequencyDataset(hyperparams[\"file_path\"])\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(hyperparams[\"train_size\"] * dataset_size)\n",
    "val_size = int(hyperparams[\"val_size\"] * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparams[\"batch_size\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams[\"batch_size\"])\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "input_dim = dataset.features.shape[1]\n",
    "model = LSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "    hidden_dim1=hyperparams[\"hidden_dim1\"],\n",
    "    hidden_dim2=hyperparams[\"hidden_dim2\"],\n",
    "    dense_units=hyperparams[\"dense_units\"],\n",
    "    num_classes=hyperparams[\"num_classes\"]\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "\n",
    "# Paths for saving models\n",
    "best_model_path = os.path.join(log_dir, \"best_model.pth\")\n",
    "latest_model_path = os.path.join(log_dir, \"latest_model.pth\")\n",
    "\n",
    "# Training and validation loop\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = hyperparams[\"patience\"]\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(hyperparams[\"num_epochs\"]):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += y_batch.size(0)\n",
    "            val_correct += (predicted == y_batch).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    logging.info(f\"Epoch [{epoch + 1}/{hyperparams['num_epochs']}] - \"\n",
    "                 f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                 f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save latest model\n",
    "    torch.save(model.state_dict(), latest_model_path)\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            logging.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "logging.info(f\"Training completed. Best model saved at {best_model_path}\")\n",
    "logging.info(f\"Latest model saved at {latest_model_path}\")\n",
    "\n",
    "# Load the best model for evaluation\n",
    "best_model = LSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "    hidden_dim1=hyperparams[\"hidden_dim1\"],\n",
    "    hidden_dim2=hyperparams[\"hidden_dim2\"],\n",
    "    dense_units=hyperparams[\"dense_units\"],\n",
    "    num_classes=hyperparams[\"num_classes\"]\n",
    ")\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.eval()\n",
    "\n",
    "# Evaluation on test set with F1-score\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(torch.float32)\n",
    "        output = best_model(X_batch)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "        test_total += y_batch.size(0)\n",
    "        test_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "f1_report = classification_report(all_labels, all_preds, digits=4)\n",
    "\n",
    "logging.info(f\"Final Test Accuracy (Best Model): {test_accuracy:.2f}%\")\n",
    "logging.info(\"Final Classification Report:\")\n",
    "logging.info(f1_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952154e-072b-4d51-8dee-136973faa01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 5 (f-1 everyepoch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set log directory\n",
    "log_dir = \"/home/user/torch_shrimp/until-tools/mod/k-mer/es/batchsig\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Ensure the log filename is the same as the directory name\n",
    "log_file = os.path.join(log_dir, f\"{os.path.basename(log_dir)}.txt\")\n",
    "\n",
    "# Remove previous logging handlers to prevent duplicate logs\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# Function to log hyperparameters\n",
    "def log_hyperparameters(hyperparams):\n",
    "    logging.info(\"Hyperparameters:\")\n",
    "    logging.info(json.dumps(hyperparams, indent=4))\n",
    "\n",
    "# Custom Dataset for k-mer frequency data\n",
    "class KmerFrequencyDataset(Dataset):\n",
    "    def __init__(self, kmer_score_file):\n",
    "        self.data = pd.read_csv(kmer_score_file)\n",
    "        self.features = self.data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "        self.labels = self.data['status'].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True, dropout=0.5)\n",
    "        self.fc1 = nn.Linear(hidden_dim2, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"embedding_dim\": 128,\n",
    "    \"hidden_dim1\": 64,\n",
    "    \"hidden_dim2\": 32,\n",
    "    \"dense_units\": 64,\n",
    "    \"num_classes\": 3,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_epochs\": 500,\n",
    "    \"batch_size\": 32,\n",
    "    \"train_size\": 0.8,\n",
    "    \"val_size\": 0.1,\n",
    "    \"file_path\": \"/home/user/torch_shrimp/until-tools/mod/k-mer/dataset/train200.csv\",\n",
    "    \"patience\": 75\n",
    "}\n",
    "log_hyperparameters(hyperparams)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = KmerFrequencyDataset(hyperparams[\"file_path\"])\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(hyperparams[\"train_size\"] * dataset_size)\n",
    "val_size = int(hyperparams[\"val_size\"] * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparams[\"batch_size\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams[\"batch_size\"])\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "input_dim = dataset.features.shape[1]\n",
    "model = LSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "    hidden_dim1=hyperparams[\"hidden_dim1\"],\n",
    "    hidden_dim2=hyperparams[\"hidden_dim2\"],\n",
    "    dense_units=hyperparams[\"dense_units\"],\n",
    "    num_classes=hyperparams[\"num_classes\"]\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "\n",
    "# Paths for saving models\n",
    "best_model_path = os.path.join(log_dir, \"best_model.pth\")\n",
    "latest_model_path = os.path.join(log_dir, \"latest_model.pth\")\n",
    "\n",
    "# Training and validation loop\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = hyperparams[\"patience\"]\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(hyperparams[\"num_epochs\"]):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += y_batch.size(0)\n",
    "            val_correct += (predicted == y_batch).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    f1_report = classification_report(all_labels, all_preds, digits=4)\n",
    "\n",
    "    logging.info(f\"Epoch [{epoch + 1}/{hyperparams['num_epochs']}] - \"\n",
    "                 f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                 f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "    logging.info(\"F1 Score:\\n\" + f1_report)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            logging.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29e793e-21ed-427f-871b-be36a86b5868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "{\n",
      "    \"embedding_dim\": 128,\n",
      "    \"hidden_dim1\": 64,\n",
      "    \"hidden_dim2\": 32,\n",
      "    \"dense_units\": 64,\n",
      "    \"num_classes\": 3,\n",
      "    \"learning_rate\": 0.0001,\n",
      "    \"num_epochs\": 1500,\n",
      "    \"batch_size\": 32,\n",
      "    \"train_size\": 0.8,\n",
      "    \"val_size\": 0.1,\n",
      "    \"file_path\": \"/home/user/torch_shrimp/until-tools/mod/k-mer/dataset/train400.csv\",\n",
      "    \"patience\": 100\n",
      "}\n",
      "/home/user/torch_shrimp/lib/python3.10/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "Epoch [1/1500] - Train Loss: 1.0980, Train Acc: 34.90%, Val Loss: 1.0992, Val Acc: 31.67%, Test Loss: 1.0983, Test Acc: 37.50%\n",
      "Epoch [2/1500] - Train Loss: 1.0992, Train Acc: 32.81%, Val Loss: 1.0992, Val Acc: 31.67%, Test Loss: 1.0983, Test Acc: 37.50%\n",
      "Epoch [3/1500] - Train Loss: 1.0989, Train Acc: 32.60%, Val Loss: 1.0993, Val Acc: 31.67%, Test Loss: 1.0983, Test Acc: 37.50%\n",
      "Epoch [4/1500] - Train Loss: 1.0983, Train Acc: 35.42%, Val Loss: 1.0993, Val Acc: 31.67%, Test Loss: 1.0983, Test Acc: 37.50%\n",
      "Epoch [5/1500] - Train Loss: 1.0980, Train Acc: 36.77%, Val Loss: 1.0993, Val Acc: 31.67%, Test Loss: 1.0982, Test Acc: 37.50%\n",
      "Epoch [6/1500] - Train Loss: 1.0986, Train Acc: 33.65%, Val Loss: 1.0993, Val Acc: 31.67%, Test Loss: 1.0982, Test Acc: 37.50%\n",
      "Epoch [7/1500] - Train Loss: 1.0989, Train Acc: 32.50%, Val Loss: 1.0993, Val Acc: 31.67%, Test Loss: 1.0982, Test Acc: 37.50%\n",
      "Epoch [8/1500] - Train Loss: 1.0987, Train Acc: 32.40%, Val Loss: 1.0994, Val Acc: 27.50%, Test Loss: 1.0982, Test Acc: 32.50%\n",
      "Epoch [9/1500] - Train Loss: 1.0982, Train Acc: 31.98%, Val Loss: 1.0994, Val Acc: 27.50%, Test Loss: 1.0981, Test Acc: 32.50%\n",
      "Epoch [10/1500] - Train Loss: 1.0976, Train Acc: 35.31%, Val Loss: 1.0994, Val Acc: 27.50%, Test Loss: 1.0981, Test Acc: 32.50%\n",
      "Epoch [11/1500] - Train Loss: 1.0983, Train Acc: 34.90%, Val Loss: 1.0993, Val Acc: 27.50%, Test Loss: 1.0980, Test Acc: 32.50%\n",
      "Epoch [12/1500] - Train Loss: 1.0985, Train Acc: 31.88%, Val Loss: 1.0992, Val Acc: 27.50%, Test Loss: 1.0979, Test Acc: 32.50%\n",
      "Epoch [13/1500] - Train Loss: 1.0978, Train Acc: 34.38%, Val Loss: 1.0991, Val Acc: 27.50%, Test Loss: 1.0977, Test Acc: 32.50%\n",
      "Epoch [14/1500] - Train Loss: 1.0973, Train Acc: 36.88%, Val Loss: 1.0990, Val Acc: 27.50%, Test Loss: 1.0973, Test Acc: 32.50%\n",
      "Epoch [15/1500] - Train Loss: 1.0969, Train Acc: 38.02%, Val Loss: 1.0984, Val Acc: 27.50%, Test Loss: 1.0968, Test Acc: 32.50%\n",
      "Epoch [16/1500] - Train Loss: 1.0965, Train Acc: 38.75%, Val Loss: 1.0977, Val Acc: 27.50%, Test Loss: 1.0958, Test Acc: 32.50%\n",
      "Epoch [17/1500] - Train Loss: 1.0945, Train Acc: 43.33%, Val Loss: 1.0962, Val Acc: 27.50%, Test Loss: 1.0939, Test Acc: 32.50%\n",
      "Epoch [18/1500] - Train Loss: 1.0927, Train Acc: 42.08%, Val Loss: 1.0931, Val Acc: 27.50%, Test Loss: 1.0903, Test Acc: 32.50%\n",
      "Epoch [19/1500] - Train Loss: 1.0870, Train Acc: 46.35%, Val Loss: 1.0872, Val Acc: 39.17%, Test Loss: 1.0837, Test Acc: 40.83%\n",
      "Epoch [20/1500] - Train Loss: 1.0771, Train Acc: 53.75%, Val Loss: 1.0779, Val Acc: 40.00%, Test Loss: 1.0722, Test Acc: 40.83%\n",
      "Epoch [21/1500] - Train Loss: 1.0596, Train Acc: 55.73%, Val Loss: 1.0569, Val Acc: 55.00%, Test Loss: 1.0510, Test Acc: 70.00%\n",
      "Epoch [22/1500] - Train Loss: 1.0330, Train Acc: 61.56%, Val Loss: 1.0289, Val Acc: 49.17%, Test Loss: 1.0179, Test Acc: 62.50%\n",
      "Epoch [23/1500] - Train Loss: 0.9947, Train Acc: 61.04%, Val Loss: 0.9928, Val Acc: 57.50%, Test Loss: 0.9800, Test Acc: 66.67%\n",
      "Epoch [24/1500] - Train Loss: 0.9564, Train Acc: 63.44%, Val Loss: 0.9614, Val Acc: 59.17%, Test Loss: 0.9481, Test Acc: 66.67%\n",
      "Epoch [25/1500] - Train Loss: 0.9279, Train Acc: 63.33%, Val Loss: 0.9427, Val Acc: 58.33%, Test Loss: 0.9278, Test Acc: 65.83%\n",
      "Epoch [26/1500] - Train Loss: 0.9096, Train Acc: 65.00%, Val Loss: 0.9296, Val Acc: 58.33%, Test Loss: 0.9130, Test Acc: 67.50%\n",
      "Epoch [27/1500] - Train Loss: 0.8985, Train Acc: 64.06%, Val Loss: 0.9138, Val Acc: 58.33%, Test Loss: 0.9003, Test Acc: 67.50%\n",
      "Epoch [28/1500] - Train Loss: 0.8887, Train Acc: 65.00%, Val Loss: 0.9093, Val Acc: 58.33%, Test Loss: 0.8940, Test Acc: 67.50%\n",
      "Epoch [29/1500] - Train Loss: 0.8809, Train Acc: 66.04%, Val Loss: 0.8983, Val Acc: 59.17%, Test Loss: 0.8848, Test Acc: 67.50%\n",
      "Epoch [30/1500] - Train Loss: 0.8751, Train Acc: 68.23%, Val Loss: 0.8923, Val Acc: 59.17%, Test Loss: 0.8789, Test Acc: 67.50%\n",
      "Epoch [31/1500] - Train Loss: 0.8713, Train Acc: 66.15%, Val Loss: 0.8948, Val Acc: 60.00%, Test Loss: 0.8781, Test Acc: 69.17%\n",
      "Epoch [32/1500] - Train Loss: 0.8656, Train Acc: 67.92%, Val Loss: 0.8856, Val Acc: 61.67%, Test Loss: 0.8690, Test Acc: 67.50%\n",
      "Epoch [33/1500] - Train Loss: 0.8601, Train Acc: 68.44%, Val Loss: 0.8820, Val Acc: 62.50%, Test Loss: 0.8648, Test Acc: 68.33%\n",
      "Epoch [34/1500] - Train Loss: 0.8600, Train Acc: 67.81%, Val Loss: 0.8791, Val Acc: 62.50%, Test Loss: 0.8613, Test Acc: 69.17%\n",
      "Epoch [35/1500] - Train Loss: 0.8573, Train Acc: 68.65%, Val Loss: 0.8761, Val Acc: 64.17%, Test Loss: 0.8590, Test Acc: 71.67%\n",
      "Epoch [36/1500] - Train Loss: 0.8514, Train Acc: 69.79%, Val Loss: 0.8713, Val Acc: 64.17%, Test Loss: 0.8518, Test Acc: 70.00%\n",
      "Epoch [37/1500] - Train Loss: 0.8504, Train Acc: 69.79%, Val Loss: 0.8680, Val Acc: 65.00%, Test Loss: 0.8492, Test Acc: 72.50%\n",
      "Epoch [38/1500] - Train Loss: 0.8476, Train Acc: 70.94%, Val Loss: 0.8649, Val Acc: 67.50%, Test Loss: 0.8484, Test Acc: 70.83%\n",
      "Epoch [39/1500] - Train Loss: 0.8474, Train Acc: 71.04%, Val Loss: 0.8623, Val Acc: 66.67%, Test Loss: 0.8453, Test Acc: 72.50%\n",
      "Epoch [40/1500] - Train Loss: 0.8387, Train Acc: 73.12%, Val Loss: 0.8566, Val Acc: 70.00%, Test Loss: 0.8375, Test Acc: 72.50%\n",
      "Epoch [41/1500] - Train Loss: 0.8360, Train Acc: 73.12%, Val Loss: 0.8518, Val Acc: 74.17%, Test Loss: 0.8359, Test Acc: 73.33%\n",
      "Epoch [42/1500] - Train Loss: 0.8262, Train Acc: 73.85%, Val Loss: 0.8451, Val Acc: 74.17%, Test Loss: 0.8309, Test Acc: 73.33%\n",
      "Epoch [43/1500] - Train Loss: 0.8227, Train Acc: 75.42%, Val Loss: 0.8385, Val Acc: 70.83%, Test Loss: 0.8208, Test Acc: 77.50%\n",
      "Epoch [44/1500] - Train Loss: 0.8206, Train Acc: 75.62%, Val Loss: 0.8324, Val Acc: 72.50%, Test Loss: 0.8141, Test Acc: 76.67%\n",
      "Epoch [45/1500] - Train Loss: 0.8096, Train Acc: 77.40%, Val Loss: 0.8232, Val Acc: 80.83%, Test Loss: 0.8085, Test Acc: 78.33%\n",
      "Epoch [46/1500] - Train Loss: 0.8054, Train Acc: 78.33%, Val Loss: 0.8137, Val Acc: 80.83%, Test Loss: 0.7973, Test Acc: 81.67%\n",
      "Epoch [47/1500] - Train Loss: 0.7912, Train Acc: 80.31%, Val Loss: 0.8077, Val Acc: 76.67%, Test Loss: 0.7886, Test Acc: 82.50%\n",
      "Epoch [48/1500] - Train Loss: 0.7837, Train Acc: 82.40%, Val Loss: 0.7981, Val Acc: 80.00%, Test Loss: 0.7789, Test Acc: 84.17%\n",
      "Epoch [49/1500] - Train Loss: 0.7751, Train Acc: 83.54%, Val Loss: 0.7859, Val Acc: 82.50%, Test Loss: 0.7695, Test Acc: 86.67%\n",
      "Epoch [50/1500] - Train Loss: 0.7621, Train Acc: 84.17%, Val Loss: 0.7791, Val Acc: 82.50%, Test Loss: 0.7600, Test Acc: 85.83%\n",
      "Epoch [51/1500] - Train Loss: 0.7598, Train Acc: 85.21%, Val Loss: 0.7698, Val Acc: 82.50%, Test Loss: 0.7517, Test Acc: 86.67%\n",
      "Epoch [52/1500] - Train Loss: 0.7513, Train Acc: 85.21%, Val Loss: 0.7661, Val Acc: 81.67%, Test Loss: 0.7464, Test Acc: 86.67%\n",
      "Epoch [53/1500] - Train Loss: 0.7358, Train Acc: 86.77%, Val Loss: 0.7586, Val Acc: 81.67%, Test Loss: 0.7396, Test Acc: 86.67%\n",
      "Epoch [54/1500] - Train Loss: 0.7306, Train Acc: 86.67%, Val Loss: 0.7556, Val Acc: 80.83%, Test Loss: 0.7374, Test Acc: 85.83%\n",
      "Epoch [55/1500] - Train Loss: 0.7309, Train Acc: 87.50%, Val Loss: 0.7433, Val Acc: 86.67%, Test Loss: 0.7251, Test Acc: 87.50%\n",
      "Epoch [56/1500] - Train Loss: 0.7155, Train Acc: 88.54%, Val Loss: 0.7412, Val Acc: 84.17%, Test Loss: 0.7247, Test Acc: 85.83%\n",
      "Epoch [57/1500] - Train Loss: 0.7115, Train Acc: 88.85%, Val Loss: 0.7333, Val Acc: 86.67%, Test Loss: 0.7160, Test Acc: 88.33%\n",
      "Epoch [58/1500] - Train Loss: 0.7117, Train Acc: 87.92%, Val Loss: 0.7278, Val Acc: 87.50%, Test Loss: 0.7113, Test Acc: 87.50%\n",
      "Epoch [59/1500] - Train Loss: 0.6978, Train Acc: 89.27%, Val Loss: 0.7221, Val Acc: 87.50%, Test Loss: 0.7080, Test Acc: 88.33%\n",
      "Epoch [60/1500] - Train Loss: 0.6952, Train Acc: 90.10%, Val Loss: 0.7209, Val Acc: 85.83%, Test Loss: 0.7072, Test Acc: 87.50%\n",
      "Epoch [61/1500] - Train Loss: 0.6955, Train Acc: 89.06%, Val Loss: 0.7145, Val Acc: 87.50%, Test Loss: 0.7032, Test Acc: 88.33%\n",
      "Epoch [62/1500] - Train Loss: 0.6921, Train Acc: 89.06%, Val Loss: 0.7115, Val Acc: 87.50%, Test Loss: 0.7011, Test Acc: 88.33%\n",
      "Epoch [63/1500] - Train Loss: 0.6883, Train Acc: 89.58%, Val Loss: 0.7074, Val Acc: 87.50%, Test Loss: 0.6985, Test Acc: 88.33%\n",
      "Epoch [64/1500] - Train Loss: 0.6809, Train Acc: 90.21%, Val Loss: 0.7036, Val Acc: 87.50%, Test Loss: 0.6946, Test Acc: 89.17%\n",
      "Epoch [65/1500] - Train Loss: 0.6842, Train Acc: 90.31%, Val Loss: 0.7002, Val Acc: 87.50%, Test Loss: 0.6923, Test Acc: 89.17%\n",
      "Epoch [66/1500] - Train Loss: 0.6762, Train Acc: 90.10%, Val Loss: 0.6980, Val Acc: 87.50%, Test Loss: 0.6912, Test Acc: 88.33%\n",
      "Epoch [67/1500] - Train Loss: 0.6765, Train Acc: 90.31%, Val Loss: 0.6950, Val Acc: 87.50%, Test Loss: 0.6889, Test Acc: 88.33%\n",
      "Epoch [68/1500] - Train Loss: 0.6674, Train Acc: 91.46%, Val Loss: 0.6939, Val Acc: 87.50%, Test Loss: 0.6867, Test Acc: 89.17%\n",
      "Epoch [69/1500] - Train Loss: 0.6687, Train Acc: 91.15%, Val Loss: 0.6908, Val Acc: 87.50%, Test Loss: 0.6844, Test Acc: 89.17%\n",
      "Epoch [70/1500] - Train Loss: 0.6660, Train Acc: 91.98%, Val Loss: 0.6881, Val Acc: 87.50%, Test Loss: 0.6832, Test Acc: 89.17%\n",
      "Epoch [71/1500] - Train Loss: 0.6647, Train Acc: 90.42%, Val Loss: 0.6865, Val Acc: 87.50%, Test Loss: 0.6821, Test Acc: 89.17%\n",
      "Epoch [72/1500] - Train Loss: 0.6595, Train Acc: 91.25%, Val Loss: 0.6848, Val Acc: 87.50%, Test Loss: 0.6819, Test Acc: 89.17%\n",
      "Epoch [73/1500] - Train Loss: 0.6644, Train Acc: 90.31%, Val Loss: 0.6829, Val Acc: 87.50%, Test Loss: 0.6806, Test Acc: 88.33%\n",
      "Epoch [74/1500] - Train Loss: 0.6575, Train Acc: 91.98%, Val Loss: 0.6815, Val Acc: 87.50%, Test Loss: 0.6779, Test Acc: 89.17%\n",
      "Epoch [75/1500] - Train Loss: 0.6533, Train Acc: 92.81%, Val Loss: 0.6802, Val Acc: 87.50%, Test Loss: 0.6760, Test Acc: 89.17%\n",
      "Epoch [76/1500] - Train Loss: 0.6522, Train Acc: 91.77%, Val Loss: 0.6796, Val Acc: 87.50%, Test Loss: 0.6751, Test Acc: 89.17%\n",
      "Epoch [77/1500] - Train Loss: 0.6524, Train Acc: 92.19%, Val Loss: 0.6784, Val Acc: 87.50%, Test Loss: 0.6742, Test Acc: 89.17%\n",
      "Epoch [78/1500] - Train Loss: 0.6508, Train Acc: 91.46%, Val Loss: 0.6766, Val Acc: 87.50%, Test Loss: 0.6729, Test Acc: 88.33%\n",
      "Epoch [79/1500] - Train Loss: 0.6423, Train Acc: 92.60%, Val Loss: 0.6753, Val Acc: 87.50%, Test Loss: 0.6715, Test Acc: 89.17%\n",
      "Epoch [80/1500] - Train Loss: 0.6495, Train Acc: 92.08%, Val Loss: 0.6751, Val Acc: 87.50%, Test Loss: 0.6705, Test Acc: 89.17%\n",
      "Epoch [81/1500] - Train Loss: 0.6433, Train Acc: 93.02%, Val Loss: 0.6735, Val Acc: 87.50%, Test Loss: 0.6710, Test Acc: 88.33%\n",
      "Epoch [82/1500] - Train Loss: 0.6434, Train Acc: 92.81%, Val Loss: 0.6733, Val Acc: 87.50%, Test Loss: 0.6689, Test Acc: 89.17%\n",
      "Epoch [83/1500] - Train Loss: 0.6474, Train Acc: 92.08%, Val Loss: 0.6713, Val Acc: 87.50%, Test Loss: 0.6675, Test Acc: 89.17%\n",
      "Epoch [84/1500] - Train Loss: 0.6403, Train Acc: 92.29%, Val Loss: 0.6712, Val Acc: 87.50%, Test Loss: 0.6671, Test Acc: 89.17%\n",
      "Epoch [85/1500] - Train Loss: 0.6411, Train Acc: 92.60%, Val Loss: 0.6695, Val Acc: 87.50%, Test Loss: 0.6677, Test Acc: 89.17%\n",
      "Epoch [86/1500] - Train Loss: 0.6399, Train Acc: 92.81%, Val Loss: 0.6694, Val Acc: 87.50%, Test Loss: 0.6663, Test Acc: 88.33%\n",
      "Epoch [87/1500] - Train Loss: 0.6401, Train Acc: 92.50%, Val Loss: 0.6684, Val Acc: 87.50%, Test Loss: 0.6645, Test Acc: 88.33%\n",
      "Epoch [88/1500] - Train Loss: 0.6361, Train Acc: 93.23%, Val Loss: 0.6679, Val Acc: 87.50%, Test Loss: 0.6647, Test Acc: 88.33%\n",
      "Epoch [89/1500] - Train Loss: 0.6334, Train Acc: 93.44%, Val Loss: 0.6682, Val Acc: 87.50%, Test Loss: 0.6637, Test Acc: 89.17%\n",
      "Epoch [90/1500] - Train Loss: 0.6324, Train Acc: 93.75%, Val Loss: 0.6658, Val Acc: 87.50%, Test Loss: 0.6628, Test Acc: 88.33%\n",
      "Epoch [91/1500] - Train Loss: 0.6341, Train Acc: 92.60%, Val Loss: 0.6656, Val Acc: 87.50%, Test Loss: 0.6621, Test Acc: 88.33%\n",
      "Epoch [92/1500] - Train Loss: 0.6322, Train Acc: 93.23%, Val Loss: 0.6658, Val Acc: 86.67%, Test Loss: 0.6614, Test Acc: 89.17%\n",
      "Epoch [93/1500] - Train Loss: 0.6300, Train Acc: 93.75%, Val Loss: 0.6626, Val Acc: 87.50%, Test Loss: 0.6591, Test Acc: 89.17%\n",
      "Epoch [94/1500] - Train Loss: 0.6335, Train Acc: 92.60%, Val Loss: 0.6640, Val Acc: 87.50%, Test Loss: 0.6594, Test Acc: 89.17%\n",
      "Epoch [95/1500] - Train Loss: 0.6290, Train Acc: 93.44%, Val Loss: 0.6647, Val Acc: 86.67%, Test Loss: 0.6599, Test Acc: 89.17%\n",
      "Epoch [96/1500] - Train Loss: 0.6246, Train Acc: 93.65%, Val Loss: 0.6627, Val Acc: 87.50%, Test Loss: 0.6591, Test Acc: 88.33%\n",
      "Epoch [97/1500] - Train Loss: 0.6290, Train Acc: 93.44%, Val Loss: 0.6646, Val Acc: 86.67%, Test Loss: 0.6598, Test Acc: 89.17%\n",
      "Epoch [98/1500] - Train Loss: 0.6270, Train Acc: 93.12%, Val Loss: 0.6648, Val Acc: 87.50%, Test Loss: 0.6591, Test Acc: 89.17%\n",
      "Epoch [99/1500] - Train Loss: 0.6244, Train Acc: 93.54%, Val Loss: 0.6639, Val Acc: 87.50%, Test Loss: 0.6582, Test Acc: 89.17%\n",
      "Epoch [100/1500] - Train Loss: 0.6266, Train Acc: 93.02%, Val Loss: 0.6617, Val Acc: 86.67%, Test Loss: 0.6570, Test Acc: 89.17%\n",
      "Epoch [101/1500] - Train Loss: 0.6257, Train Acc: 93.33%, Val Loss: 0.6573, Val Acc: 88.33%, Test Loss: 0.6555, Test Acc: 90.00%\n",
      "Epoch [102/1500] - Train Loss: 0.6238, Train Acc: 93.85%, Val Loss: 0.6604, Val Acc: 86.67%, Test Loss: 0.6555, Test Acc: 90.00%\n",
      "Epoch [103/1500] - Train Loss: 0.6231, Train Acc: 93.96%, Val Loss: 0.6598, Val Acc: 86.67%, Test Loss: 0.6540, Test Acc: 90.00%\n",
      "Epoch [104/1500] - Train Loss: 0.6282, Train Acc: 93.75%, Val Loss: 0.6561, Val Acc: 88.33%, Test Loss: 0.6529, Test Acc: 91.67%\n",
      "Epoch [105/1500] - Train Loss: 0.6201, Train Acc: 93.85%, Val Loss: 0.6605, Val Acc: 87.50%, Test Loss: 0.6531, Test Acc: 90.00%\n",
      "Epoch [106/1500] - Train Loss: 0.6213, Train Acc: 94.06%, Val Loss: 0.6548, Val Acc: 89.17%, Test Loss: 0.6530, Test Acc: 90.83%\n",
      "Epoch [107/1500] - Train Loss: 0.6195, Train Acc: 94.90%, Val Loss: 0.6587, Val Acc: 87.50%, Test Loss: 0.6503, Test Acc: 90.00%\n",
      "Epoch [108/1500] - Train Loss: 0.6164, Train Acc: 94.38%, Val Loss: 0.6574, Val Acc: 86.67%, Test Loss: 0.6509, Test Acc: 90.83%\n",
      "Epoch [109/1500] - Train Loss: 0.6251, Train Acc: 93.44%, Val Loss: 0.6517, Val Acc: 89.17%, Test Loss: 0.6542, Test Acc: 90.00%\n",
      "Epoch [110/1500] - Train Loss: 0.6188, Train Acc: 94.17%, Val Loss: 0.6570, Val Acc: 87.50%, Test Loss: 0.6516, Test Acc: 91.67%\n",
      "Epoch [111/1500] - Train Loss: 0.6214, Train Acc: 93.75%, Val Loss: 0.6571, Val Acc: 86.67%, Test Loss: 0.6499, Test Acc: 90.83%\n",
      "Epoch [112/1500] - Train Loss: 0.6181, Train Acc: 93.75%, Val Loss: 0.6611, Val Acc: 87.50%, Test Loss: 0.6523, Test Acc: 90.00%\n",
      "Epoch [113/1500] - Train Loss: 0.6179, Train Acc: 93.85%, Val Loss: 0.6606, Val Acc: 87.50%, Test Loss: 0.6507, Test Acc: 90.00%\n",
      "Epoch [114/1500] - Train Loss: 0.6188, Train Acc: 93.75%, Val Loss: 0.6516, Val Acc: 89.17%, Test Loss: 0.6514, Test Acc: 90.83%\n",
      "Epoch [115/1500] - Train Loss: 0.6204, Train Acc: 94.17%, Val Loss: 0.6581, Val Acc: 87.50%, Test Loss: 0.6478, Test Acc: 90.00%\n",
      "Epoch [116/1500] - Train Loss: 0.6203, Train Acc: 93.33%, Val Loss: 0.6590, Val Acc: 87.50%, Test Loss: 0.6491, Test Acc: 90.00%\n",
      "Epoch [117/1500] - Train Loss: 0.6124, Train Acc: 95.00%, Val Loss: 0.6509, Val Acc: 88.33%, Test Loss: 0.6504, Test Acc: 90.83%\n",
      "Epoch [118/1500] - Train Loss: 0.6152, Train Acc: 94.27%, Val Loss: 0.6555, Val Acc: 88.33%, Test Loss: 0.6472, Test Acc: 91.67%\n",
      "Epoch [119/1500] - Train Loss: 0.6159, Train Acc: 94.27%, Val Loss: 0.6522, Val Acc: 88.33%, Test Loss: 0.6491, Test Acc: 91.67%\n",
      "Epoch [120/1500] - Train Loss: 0.6114, Train Acc: 94.58%, Val Loss: 0.6579, Val Acc: 87.50%, Test Loss: 0.6495, Test Acc: 90.83%\n",
      "Epoch [121/1500] - Train Loss: 0.6156, Train Acc: 93.96%, Val Loss: 0.6509, Val Acc: 88.33%, Test Loss: 0.6489, Test Acc: 91.67%\n",
      "Epoch [122/1500] - Train Loss: 0.6118, Train Acc: 94.79%, Val Loss: 0.6523, Val Acc: 88.33%, Test Loss: 0.6461, Test Acc: 91.67%\n",
      "Epoch [123/1500] - Train Loss: 0.6133, Train Acc: 94.58%, Val Loss: 0.6503, Val Acc: 88.33%, Test Loss: 0.6477, Test Acc: 91.67%\n",
      "Epoch [124/1500] - Train Loss: 0.6119, Train Acc: 94.79%, Val Loss: 0.6560, Val Acc: 88.33%, Test Loss: 0.6472, Test Acc: 90.83%\n",
      "Epoch [125/1500] - Train Loss: 0.6106, Train Acc: 94.48%, Val Loss: 0.6539, Val Acc: 88.33%, Test Loss: 0.6455, Test Acc: 91.67%\n",
      "Epoch [126/1500] - Train Loss: 0.6182, Train Acc: 93.75%, Val Loss: 0.6521, Val Acc: 88.33%, Test Loss: 0.6459, Test Acc: 91.67%\n",
      "Epoch [127/1500] - Train Loss: 0.6118, Train Acc: 94.69%, Val Loss: 0.6508, Val Acc: 88.33%, Test Loss: 0.6468, Test Acc: 91.67%\n",
      "Epoch [128/1500] - Train Loss: 0.6131, Train Acc: 94.58%, Val Loss: 0.6537, Val Acc: 88.33%, Test Loss: 0.6457, Test Acc: 91.67%\n",
      "Epoch [129/1500] - Train Loss: 0.6147, Train Acc: 94.38%, Val Loss: 0.6525, Val Acc: 88.33%, Test Loss: 0.6449, Test Acc: 91.67%\n",
      "Epoch [130/1500] - Train Loss: 0.6111, Train Acc: 94.38%, Val Loss: 0.6535, Val Acc: 88.33%, Test Loss: 0.6445, Test Acc: 91.67%\n",
      "Epoch [131/1500] - Train Loss: 0.6114, Train Acc: 94.69%, Val Loss: 0.6540, Val Acc: 88.33%, Test Loss: 0.6444, Test Acc: 91.67%\n",
      "Epoch [132/1500] - Train Loss: 0.6069, Train Acc: 94.79%, Val Loss: 0.6516, Val Acc: 88.33%, Test Loss: 0.6437, Test Acc: 91.67%\n",
      "Epoch [133/1500] - Train Loss: 0.6119, Train Acc: 94.48%, Val Loss: 0.6524, Val Acc: 88.33%, Test Loss: 0.6434, Test Acc: 91.67%\n",
      "Epoch [134/1500] - Train Loss: 0.6080, Train Acc: 94.90%, Val Loss: 0.6491, Val Acc: 89.17%, Test Loss: 0.6443, Test Acc: 91.67%\n",
      "Epoch [135/1500] - Train Loss: 0.6097, Train Acc: 94.38%, Val Loss: 0.6505, Val Acc: 89.17%, Test Loss: 0.6456, Test Acc: 91.67%\n",
      "Epoch [136/1500] - Train Loss: 0.6121, Train Acc: 94.17%, Val Loss: 0.6533, Val Acc: 88.33%, Test Loss: 0.6424, Test Acc: 91.67%\n",
      "Epoch [137/1500] - Train Loss: 0.6110, Train Acc: 94.69%, Val Loss: 0.6416, Val Acc: 90.00%, Test Loss: 0.6472, Test Acc: 90.83%\n",
      "Epoch [138/1500] - Train Loss: 0.6107, Train Acc: 94.48%, Val Loss: 0.6557, Val Acc: 87.50%, Test Loss: 0.6448, Test Acc: 90.83%\n",
      "Epoch [139/1500] - Train Loss: 0.6068, Train Acc: 95.21%, Val Loss: 0.6485, Val Acc: 89.17%, Test Loss: 0.6429, Test Acc: 91.67%\n",
      "Epoch [140/1500] - Train Loss: 0.6069, Train Acc: 94.90%, Val Loss: 0.6515, Val Acc: 88.33%, Test Loss: 0.6427, Test Acc: 91.67%\n",
      "Epoch [141/1500] - Train Loss: 0.6063, Train Acc: 94.90%, Val Loss: 0.6487, Val Acc: 89.17%, Test Loss: 0.6428, Test Acc: 91.67%\n",
      "Epoch [142/1500] - Train Loss: 0.6056, Train Acc: 95.10%, Val Loss: 0.6468, Val Acc: 89.17%, Test Loss: 0.6429, Test Acc: 91.67%\n",
      "Epoch [143/1500] - Train Loss: 0.6084, Train Acc: 94.90%, Val Loss: 0.6480, Val Acc: 89.17%, Test Loss: 0.6415, Test Acc: 91.67%\n",
      "Epoch [144/1500] - Train Loss: 0.6098, Train Acc: 94.69%, Val Loss: 0.6468, Val Acc: 89.17%, Test Loss: 0.6410, Test Acc: 91.67%\n",
      "Epoch [145/1500] - Train Loss: 0.6062, Train Acc: 95.10%, Val Loss: 0.6507, Val Acc: 88.33%, Test Loss: 0.6393, Test Acc: 91.67%\n",
      "Epoch [146/1500] - Train Loss: 0.6059, Train Acc: 95.10%, Val Loss: 0.6442, Val Acc: 89.17%, Test Loss: 0.6424, Test Acc: 91.67%\n",
      "Epoch [147/1500] - Train Loss: 0.6041, Train Acc: 95.21%, Val Loss: 0.6397, Val Acc: 89.17%, Test Loss: 0.6441, Test Acc: 90.83%\n",
      "Epoch [148/1500] - Train Loss: 0.6077, Train Acc: 94.90%, Val Loss: 0.6441, Val Acc: 89.17%, Test Loss: 0.6427, Test Acc: 90.83%\n",
      "Epoch [149/1500] - Train Loss: 0.6056, Train Acc: 95.21%, Val Loss: 0.6470, Val Acc: 89.17%, Test Loss: 0.6405, Test Acc: 91.67%\n",
      "Epoch [150/1500] - Train Loss: 0.6056, Train Acc: 95.10%, Val Loss: 0.6426, Val Acc: 89.17%, Test Loss: 0.6420, Test Acc: 91.67%\n",
      "Epoch [151/1500] - Train Loss: 0.6044, Train Acc: 95.31%, Val Loss: 0.6449, Val Acc: 89.17%, Test Loss: 0.6404, Test Acc: 91.67%\n",
      "Epoch [152/1500] - Train Loss: 0.5990, Train Acc: 95.73%, Val Loss: 0.6382, Val Acc: 90.83%, Test Loss: 0.6444, Test Acc: 90.83%\n",
      "Epoch [153/1500] - Train Loss: 0.6031, Train Acc: 95.10%, Val Loss: 0.6413, Val Acc: 90.83%, Test Loss: 0.6441, Test Acc: 90.83%\n",
      "Epoch [154/1500] - Train Loss: 0.6030, Train Acc: 95.42%, Val Loss: 0.6412, Val Acc: 90.83%, Test Loss: 0.6428, Test Acc: 90.83%\n",
      "Epoch [155/1500] - Train Loss: 0.6051, Train Acc: 95.00%, Val Loss: 0.6461, Val Acc: 89.17%, Test Loss: 0.6384, Test Acc: 91.67%\n",
      "Epoch [156/1500] - Train Loss: 0.6022, Train Acc: 95.31%, Val Loss: 0.6410, Val Acc: 89.17%, Test Loss: 0.6409, Test Acc: 91.67%\n",
      "Epoch [157/1500] - Train Loss: 0.6008, Train Acc: 95.31%, Val Loss: 0.6403, Val Acc: 89.17%, Test Loss: 0.6403, Test Acc: 91.67%\n",
      "Epoch [158/1500] - Train Loss: 0.6000, Train Acc: 96.04%, Val Loss: 0.6378, Val Acc: 89.17%, Test Loss: 0.6418, Test Acc: 90.83%\n",
      "Epoch [159/1500] - Train Loss: 0.6019, Train Acc: 95.42%, Val Loss: 0.6441, Val Acc: 89.17%, Test Loss: 0.6389, Test Acc: 91.67%\n",
      "Epoch [160/1500] - Train Loss: 0.5990, Train Acc: 95.31%, Val Loss: 0.6446, Val Acc: 89.17%, Test Loss: 0.6383, Test Acc: 91.67%\n",
      "Epoch [161/1500] - Train Loss: 0.6020, Train Acc: 95.10%, Val Loss: 0.6403, Val Acc: 89.17%, Test Loss: 0.6404, Test Acc: 90.83%\n",
      "Epoch [162/1500] - Train Loss: 0.6022, Train Acc: 95.73%, Val Loss: 0.6395, Val Acc: 89.17%, Test Loss: 0.6400, Test Acc: 90.83%\n",
      "Epoch [163/1500] - Train Loss: 0.5993, Train Acc: 95.62%, Val Loss: 0.6309, Val Acc: 91.67%, Test Loss: 0.6467, Test Acc: 90.00%\n",
      "Epoch [164/1500] - Train Loss: 0.5997, Train Acc: 95.62%, Val Loss: 0.6389, Val Acc: 89.17%, Test Loss: 0.6393, Test Acc: 90.83%\n",
      "Epoch [165/1500] - Train Loss: 0.6004, Train Acc: 95.42%, Val Loss: 0.6427, Val Acc: 89.17%, Test Loss: 0.6387, Test Acc: 91.67%\n",
      "Epoch [166/1500] - Train Loss: 0.5998, Train Acc: 95.52%, Val Loss: 0.6410, Val Acc: 89.17%, Test Loss: 0.6382, Test Acc: 91.67%\n",
      "Epoch [167/1500] - Train Loss: 0.5982, Train Acc: 95.62%, Val Loss: 0.6332, Val Acc: 90.00%, Test Loss: 0.6411, Test Acc: 90.83%\n",
      "Epoch [168/1500] - Train Loss: 0.6002, Train Acc: 95.62%, Val Loss: 0.6436, Val Acc: 89.17%, Test Loss: 0.6360, Test Acc: 91.67%\n",
      "Epoch [169/1500] - Train Loss: 0.5966, Train Acc: 95.83%, Val Loss: 0.6351, Val Acc: 90.00%, Test Loss: 0.6380, Test Acc: 91.67%\n",
      "Epoch [170/1500] - Train Loss: 0.5952, Train Acc: 96.35%, Val Loss: 0.6339, Val Acc: 90.00%, Test Loss: 0.6389, Test Acc: 90.83%\n",
      "Epoch [171/1500] - Train Loss: 0.5989, Train Acc: 95.31%, Val Loss: 0.6350, Val Acc: 90.00%, Test Loss: 0.6377, Test Acc: 91.67%\n",
      "Epoch [172/1500] - Train Loss: 0.5988, Train Acc: 95.42%, Val Loss: 0.6422, Val Acc: 89.17%, Test Loss: 0.6356, Test Acc: 91.67%\n",
      "Epoch [173/1500] - Train Loss: 0.5956, Train Acc: 96.04%, Val Loss: 0.6271, Val Acc: 90.83%, Test Loss: 0.6410, Test Acc: 90.83%\n",
      "Epoch [174/1500] - Train Loss: 0.5993, Train Acc: 95.42%, Val Loss: 0.6310, Val Acc: 90.00%, Test Loss: 0.6392, Test Acc: 90.83%\n",
      "Epoch [175/1500] - Train Loss: 0.5982, Train Acc: 95.62%, Val Loss: 0.6373, Val Acc: 90.00%, Test Loss: 0.6372, Test Acc: 91.67%\n",
      "Epoch [176/1500] - Train Loss: 0.5949, Train Acc: 96.25%, Val Loss: 0.6245, Val Acc: 93.33%, Test Loss: 0.6430, Test Acc: 90.83%\n",
      "Epoch [177/1500] - Train Loss: 0.5959, Train Acc: 95.83%, Val Loss: 0.6371, Val Acc: 90.00%, Test Loss: 0.6369, Test Acc: 91.67%\n",
      "Epoch [178/1500] - Train Loss: 0.5964, Train Acc: 95.83%, Val Loss: 0.6320, Val Acc: 90.00%, Test Loss: 0.6387, Test Acc: 90.83%\n",
      "Epoch [179/1500] - Train Loss: 0.5955, Train Acc: 96.15%, Val Loss: 0.6297, Val Acc: 90.83%, Test Loss: 0.6396, Test Acc: 90.83%\n",
      "Epoch [180/1500] - Train Loss: 0.5980, Train Acc: 95.73%, Val Loss: 0.6299, Val Acc: 90.00%, Test Loss: 0.6385, Test Acc: 90.83%\n",
      "Epoch [181/1500] - Train Loss: 0.5973, Train Acc: 95.52%, Val Loss: 0.6415, Val Acc: 89.17%, Test Loss: 0.6324, Test Acc: 92.50%\n",
      "Epoch [182/1500] - Train Loss: 0.5929, Train Acc: 96.46%, Val Loss: 0.6280, Val Acc: 90.83%, Test Loss: 0.6385, Test Acc: 90.83%\n",
      "Epoch [183/1500] - Train Loss: 0.5985, Train Acc: 95.42%, Val Loss: 0.6278, Val Acc: 90.83%, Test Loss: 0.6383, Test Acc: 90.83%\n",
      "Epoch [184/1500] - Train Loss: 0.5918, Train Acc: 96.46%, Val Loss: 0.6334, Val Acc: 90.83%, Test Loss: 0.6349, Test Acc: 92.50%\n",
      "Epoch [185/1500] - Train Loss: 0.5963, Train Acc: 95.73%, Val Loss: 0.6306, Val Acc: 90.83%, Test Loss: 0.6363, Test Acc: 91.67%\n",
      "Epoch [186/1500] - Train Loss: 0.5942, Train Acc: 96.25%, Val Loss: 0.6354, Val Acc: 90.00%, Test Loss: 0.6350, Test Acc: 91.67%\n",
      "Epoch [187/1500] - Train Loss: 0.5958, Train Acc: 96.04%, Val Loss: 0.6278, Val Acc: 90.00%, Test Loss: 0.6386, Test Acc: 90.83%\n",
      "Epoch [188/1500] - Train Loss: 0.5950, Train Acc: 96.15%, Val Loss: 0.6220, Val Acc: 93.33%, Test Loss: 0.6412, Test Acc: 90.83%\n",
      "Epoch [189/1500] - Train Loss: 0.5905, Train Acc: 96.56%, Val Loss: 0.6228, Val Acc: 93.33%, Test Loss: 0.6410, Test Acc: 90.83%\n",
      "Epoch [190/1500] - Train Loss: 0.5928, Train Acc: 96.25%, Val Loss: 0.6223, Val Acc: 93.33%, Test Loss: 0.6410, Test Acc: 90.83%\n",
      "Epoch [191/1500] - Train Loss: 0.5925, Train Acc: 96.25%, Val Loss: 0.6277, Val Acc: 90.83%, Test Loss: 0.6365, Test Acc: 91.67%\n",
      "Epoch [192/1500] - Train Loss: 0.5923, Train Acc: 96.35%, Val Loss: 0.6225, Val Acc: 92.50%, Test Loss: 0.6409, Test Acc: 90.00%\n",
      "Epoch [193/1500] - Train Loss: 0.5929, Train Acc: 96.25%, Val Loss: 0.6169, Val Acc: 93.33%, Test Loss: 0.6432, Test Acc: 89.17%\n",
      "Epoch [194/1500] - Train Loss: 0.5926, Train Acc: 96.35%, Val Loss: 0.6260, Val Acc: 91.67%, Test Loss: 0.6378, Test Acc: 90.83%\n",
      "Epoch [195/1500] - Train Loss: 0.5917, Train Acc: 96.46%, Val Loss: 0.6292, Val Acc: 90.83%, Test Loss: 0.6357, Test Acc: 91.67%\n",
      "Epoch [196/1500] - Train Loss: 0.5931, Train Acc: 96.04%, Val Loss: 0.6335, Val Acc: 90.00%, Test Loss: 0.6339, Test Acc: 91.67%\n",
      "Epoch [197/1500] - Train Loss: 0.5916, Train Acc: 96.35%, Val Loss: 0.6214, Val Acc: 93.33%, Test Loss: 0.6398, Test Acc: 90.83%\n",
      "Epoch [198/1500] - Train Loss: 0.5929, Train Acc: 96.25%, Val Loss: 0.6212, Val Acc: 93.33%, Test Loss: 0.6395, Test Acc: 90.83%\n",
      "Epoch [199/1500] - Train Loss: 0.5908, Train Acc: 96.56%, Val Loss: 0.6312, Val Acc: 90.00%, Test Loss: 0.6345, Test Acc: 91.67%\n",
      "Epoch [200/1500] - Train Loss: 0.5902, Train Acc: 96.56%, Val Loss: 0.6291, Val Acc: 90.00%, Test Loss: 0.6348, Test Acc: 91.67%\n",
      "Epoch [201/1500] - Train Loss: 0.5921, Train Acc: 96.46%, Val Loss: 0.6254, Val Acc: 92.50%, Test Loss: 0.6370, Test Acc: 91.67%\n",
      "Epoch [202/1500] - Train Loss: 0.5924, Train Acc: 96.25%, Val Loss: 0.6241, Val Acc: 93.33%, Test Loss: 0.6373, Test Acc: 91.67%\n",
      "Epoch [203/1500] - Train Loss: 0.5898, Train Acc: 96.46%, Val Loss: 0.6229, Val Acc: 93.33%, Test Loss: 0.6377, Test Acc: 91.67%\n",
      "Epoch [204/1500] - Train Loss: 0.5884, Train Acc: 96.77%, Val Loss: 0.6299, Val Acc: 90.83%, Test Loss: 0.6336, Test Acc: 91.67%\n",
      "Epoch [205/1500] - Train Loss: 0.5887, Train Acc: 96.98%, Val Loss: 0.6199, Val Acc: 93.33%, Test Loss: 0.6392, Test Acc: 90.83%\n",
      "Epoch [206/1500] - Train Loss: 0.5887, Train Acc: 96.98%, Val Loss: 0.6138, Val Acc: 93.33%, Test Loss: 0.6433, Test Acc: 89.17%\n",
      "Epoch [207/1500] - Train Loss: 0.5895, Train Acc: 96.67%, Val Loss: 0.6284, Val Acc: 90.83%, Test Loss: 0.6337, Test Acc: 91.67%\n",
      "Epoch [208/1500] - Train Loss: 0.5911, Train Acc: 96.25%, Val Loss: 0.6234, Val Acc: 93.33%, Test Loss: 0.6368, Test Acc: 91.67%\n",
      "Epoch [209/1500] - Train Loss: 0.5892, Train Acc: 96.88%, Val Loss: 0.6210, Val Acc: 93.33%, Test Loss: 0.6386, Test Acc: 90.83%\n",
      "Epoch [210/1500] - Train Loss: 0.5929, Train Acc: 96.04%, Val Loss: 0.6157, Val Acc: 93.33%, Test Loss: 0.6417, Test Acc: 89.17%\n",
      "Epoch [211/1500] - Train Loss: 0.5879, Train Acc: 96.88%, Val Loss: 0.6225, Val Acc: 93.33%, Test Loss: 0.6367, Test Acc: 91.67%\n",
      "Epoch [212/1500] - Train Loss: 0.5891, Train Acc: 96.77%, Val Loss: 0.6160, Val Acc: 93.33%, Test Loss: 0.6412, Test Acc: 89.17%\n",
      "Epoch [213/1500] - Train Loss: 0.5911, Train Acc: 96.35%, Val Loss: 0.6178, Val Acc: 93.33%, Test Loss: 0.6394, Test Acc: 90.00%\n",
      "Epoch [214/1500] - Train Loss: 0.5883, Train Acc: 96.77%, Val Loss: 0.6289, Val Acc: 90.83%, Test Loss: 0.6324, Test Acc: 92.50%\n",
      "Epoch [215/1500] - Train Loss: 0.5874, Train Acc: 96.67%, Val Loss: 0.6139, Val Acc: 94.17%, Test Loss: 0.6413, Test Acc: 89.17%\n",
      "Epoch [216/1500] - Train Loss: 0.5904, Train Acc: 96.35%, Val Loss: 0.6227, Val Acc: 93.33%, Test Loss: 0.6356, Test Acc: 91.67%\n",
      "Epoch [217/1500] - Train Loss: 0.5891, Train Acc: 96.25%, Val Loss: 0.6286, Val Acc: 90.00%, Test Loss: 0.6321, Test Acc: 91.67%\n",
      "Epoch [218/1500] - Train Loss: 0.5893, Train Acc: 96.15%, Val Loss: 0.6121, Val Acc: 93.33%, Test Loss: 0.6425, Test Acc: 90.00%\n",
      "Epoch [219/1500] - Train Loss: 0.5881, Train Acc: 96.88%, Val Loss: 0.6226, Val Acc: 93.33%, Test Loss: 0.6350, Test Acc: 91.67%\n",
      "Epoch [220/1500] - Train Loss: 0.5870, Train Acc: 96.77%, Val Loss: 0.6136, Val Acc: 93.33%, Test Loss: 0.6412, Test Acc: 90.00%\n",
      "Epoch [221/1500] - Train Loss: 0.5868, Train Acc: 96.77%, Val Loss: 0.6175, Val Acc: 93.33%, Test Loss: 0.6380, Test Acc: 90.83%\n",
      "Epoch [222/1500] - Train Loss: 0.5880, Train Acc: 96.56%, Val Loss: 0.6222, Val Acc: 91.67%, Test Loss: 0.6348, Test Acc: 90.83%\n",
      "Epoch [223/1500] - Train Loss: 0.5883, Train Acc: 96.46%, Val Loss: 0.6112, Val Acc: 93.33%, Test Loss: 0.6423, Test Acc: 89.17%\n",
      "Epoch [224/1500] - Train Loss: 0.5903, Train Acc: 96.35%, Val Loss: 0.6212, Val Acc: 93.33%, Test Loss: 0.6350, Test Acc: 91.67%\n",
      "Epoch [225/1500] - Train Loss: 0.5902, Train Acc: 96.46%, Val Loss: 0.6198, Val Acc: 92.50%, Test Loss: 0.6362, Test Acc: 90.83%\n",
      "Epoch [226/1500] - Train Loss: 0.5879, Train Acc: 96.56%, Val Loss: 0.6140, Val Acc: 93.33%, Test Loss: 0.6399, Test Acc: 90.00%\n",
      "Epoch [227/1500] - Train Loss: 0.5869, Train Acc: 96.67%, Val Loss: 0.6194, Val Acc: 91.67%, Test Loss: 0.6358, Test Acc: 90.83%\n",
      "Epoch [228/1500] - Train Loss: 0.5858, Train Acc: 96.98%, Val Loss: 0.6281, Val Acc: 91.67%, Test Loss: 0.6303, Test Acc: 92.50%\n",
      "Epoch [229/1500] - Train Loss: 0.5871, Train Acc: 96.56%, Val Loss: 0.6219, Val Acc: 93.33%, Test Loss: 0.6336, Test Acc: 91.67%\n",
      "Epoch [230/1500] - Train Loss: 0.5862, Train Acc: 96.98%, Val Loss: 0.6267, Val Acc: 91.67%, Test Loss: 0.6307, Test Acc: 92.50%\n",
      "Epoch [231/1500] - Train Loss: 0.5879, Train Acc: 96.46%, Val Loss: 0.6222, Val Acc: 92.50%, Test Loss: 0.6330, Test Acc: 91.67%\n",
      "Epoch [232/1500] - Train Loss: 0.5860, Train Acc: 96.88%, Val Loss: 0.6218, Val Acc: 92.50%, Test Loss: 0.6332, Test Acc: 91.67%\n",
      "Epoch [233/1500] - Train Loss: 0.5882, Train Acc: 96.56%, Val Loss: 0.6141, Val Acc: 93.33%, Test Loss: 0.6380, Test Acc: 90.83%\n",
      "Epoch [234/1500] - Train Loss: 0.5848, Train Acc: 97.08%, Val Loss: 0.6244, Val Acc: 92.50%, Test Loss: 0.6315, Test Acc: 92.50%\n",
      "Epoch [235/1500] - Train Loss: 0.5875, Train Acc: 96.56%, Val Loss: 0.6265, Val Acc: 90.83%, Test Loss: 0.6296, Test Acc: 92.50%\n",
      "Epoch [236/1500] - Train Loss: 0.5836, Train Acc: 96.98%, Val Loss: 0.6205, Val Acc: 93.33%, Test Loss: 0.6331, Test Acc: 91.67%\n",
      "Epoch [237/1500] - Train Loss: 0.5867, Train Acc: 96.77%, Val Loss: 0.6209, Val Acc: 93.33%, Test Loss: 0.6326, Test Acc: 91.67%\n",
      "Epoch [238/1500] - Train Loss: 0.5861, Train Acc: 97.08%, Val Loss: 0.6096, Val Acc: 93.33%, Test Loss: 0.6399, Test Acc: 90.83%\n",
      "Epoch [239/1500] - Train Loss: 0.5864, Train Acc: 96.88%, Val Loss: 0.6184, Val Acc: 93.33%, Test Loss: 0.6330, Test Acc: 91.67%\n",
      "Epoch [240/1500] - Train Loss: 0.5858, Train Acc: 97.08%, Val Loss: 0.6065, Val Acc: 94.17%, Test Loss: 0.6444, Test Acc: 90.00%\n",
      "Epoch [241/1500] - Train Loss: 0.5872, Train Acc: 96.56%, Val Loss: 0.6114, Val Acc: 93.33%, Test Loss: 0.6380, Test Acc: 90.83%\n",
      "Epoch [242/1500] - Train Loss: 0.5842, Train Acc: 97.08%, Val Loss: 0.6257, Val Acc: 91.67%, Test Loss: 0.6280, Test Acc: 92.50%\n",
      "Epoch [243/1500] - Train Loss: 0.5856, Train Acc: 96.98%, Val Loss: 0.6158, Val Acc: 93.33%, Test Loss: 0.6340, Test Acc: 90.83%\n",
      "Epoch [244/1500] - Train Loss: 0.5817, Train Acc: 97.29%, Val Loss: 0.6199, Val Acc: 93.33%, Test Loss: 0.6308, Test Acc: 92.50%\n",
      "Epoch [245/1500] - Train Loss: 0.5834, Train Acc: 97.29%, Val Loss: 0.6217, Val Acc: 92.50%, Test Loss: 0.6295, Test Acc: 92.50%\n",
      "Epoch [246/1500] - Train Loss: 0.5858, Train Acc: 96.98%, Val Loss: 0.6135, Val Acc: 93.33%, Test Loss: 0.6353, Test Acc: 90.83%\n",
      "Epoch [247/1500] - Train Loss: 0.5819, Train Acc: 97.40%, Val Loss: 0.6119, Val Acc: 93.33%, Test Loss: 0.6361, Test Acc: 90.83%\n",
      "Epoch [248/1500] - Train Loss: 0.5860, Train Acc: 96.56%, Val Loss: 0.6198, Val Acc: 93.33%, Test Loss: 0.6299, Test Acc: 92.50%\n",
      "Epoch [249/1500] - Train Loss: 0.5859, Train Acc: 96.98%, Val Loss: 0.6229, Val Acc: 91.67%, Test Loss: 0.6274, Test Acc: 92.50%\n",
      "Epoch [250/1500] - Train Loss: 0.5831, Train Acc: 97.29%, Val Loss: 0.6139, Val Acc: 93.33%, Test Loss: 0.6331, Test Acc: 91.67%\n",
      "Epoch [251/1500] - Train Loss: 0.5831, Train Acc: 97.19%, Val Loss: 0.6154, Val Acc: 93.33%, Test Loss: 0.6317, Test Acc: 92.50%\n",
      "Epoch [252/1500] - Train Loss: 0.5862, Train Acc: 96.77%, Val Loss: 0.6124, Val Acc: 93.33%, Test Loss: 0.6338, Test Acc: 90.83%\n",
      "Epoch [253/1500] - Train Loss: 0.5848, Train Acc: 96.67%, Val Loss: 0.6159, Val Acc: 93.33%, Test Loss: 0.6305, Test Acc: 92.50%\n",
      "Epoch [254/1500] - Train Loss: 0.5840, Train Acc: 97.19%, Val Loss: 0.6062, Val Acc: 94.17%, Test Loss: 0.6420, Test Acc: 90.00%\n",
      "Epoch [255/1500] - Train Loss: 0.5841, Train Acc: 96.88%, Val Loss: 0.6091, Val Acc: 94.17%, Test Loss: 0.6403, Test Acc: 90.00%\n",
      "Epoch [256/1500] - Train Loss: 0.5829, Train Acc: 97.08%, Val Loss: 0.6147, Val Acc: 93.33%, Test Loss: 0.6312, Test Acc: 92.50%\n",
      "Epoch [257/1500] - Train Loss: 0.5837, Train Acc: 96.88%, Val Loss: 0.6041, Val Acc: 95.00%, Test Loss: 0.6400, Test Acc: 90.83%\n",
      "Epoch [258/1500] - Train Loss: 0.5847, Train Acc: 96.98%, Val Loss: 0.6117, Val Acc: 93.33%, Test Loss: 0.6336, Test Acc: 91.67%\n",
      "Epoch [259/1500] - Train Loss: 0.5824, Train Acc: 97.08%, Val Loss: 0.6174, Val Acc: 93.33%, Test Loss: 0.6283, Test Acc: 92.50%\n",
      "Epoch [260/1500] - Train Loss: 0.5832, Train Acc: 96.98%, Val Loss: 0.6101, Val Acc: 93.33%, Test Loss: 0.6337, Test Acc: 91.67%\n",
      "Epoch [261/1500] - Train Loss: 0.5841, Train Acc: 96.88%, Val Loss: 0.6122, Val Acc: 93.33%, Test Loss: 0.6310, Test Acc: 91.67%\n",
      "Epoch [262/1500] - Train Loss: 0.5819, Train Acc: 97.50%, Val Loss: 0.6151, Val Acc: 93.33%, Test Loss: 0.6285, Test Acc: 92.50%\n",
      "Epoch [263/1500] - Train Loss: 0.5840, Train Acc: 97.08%, Val Loss: 0.6189, Val Acc: 93.33%, Test Loss: 0.6254, Test Acc: 91.67%\n",
      "Epoch [264/1500] - Train Loss: 0.5818, Train Acc: 97.08%, Val Loss: 0.6116, Val Acc: 93.33%, Test Loss: 0.6343, Test Acc: 90.83%\n",
      "Epoch [265/1500] - Train Loss: 0.5818, Train Acc: 96.98%, Val Loss: 0.6146, Val Acc: 93.33%, Test Loss: 0.6258, Test Acc: 91.67%\n",
      "Epoch [266/1500] - Train Loss: 0.5837, Train Acc: 97.08%, Val Loss: 0.6112, Val Acc: 93.33%, Test Loss: 0.6284, Test Acc: 92.50%\n",
      "Epoch [267/1500] - Train Loss: 0.5806, Train Acc: 97.29%, Val Loss: 0.6181, Val Acc: 93.33%, Test Loss: 0.6227, Test Acc: 91.67%\n",
      "Epoch [268/1500] - Train Loss: 0.5827, Train Acc: 96.88%, Val Loss: 0.6133, Val Acc: 93.33%, Test Loss: 0.6246, Test Acc: 92.50%\n",
      "Epoch [269/1500] - Train Loss: 0.5811, Train Acc: 97.29%, Val Loss: 0.6129, Val Acc: 93.33%, Test Loss: 0.6268, Test Acc: 91.67%\n",
      "Epoch [270/1500] - Train Loss: 0.5802, Train Acc: 97.29%, Val Loss: 0.6162, Val Acc: 93.33%, Test Loss: 0.6231, Test Acc: 92.50%\n",
      "Epoch [271/1500] - Train Loss: 0.5834, Train Acc: 97.19%, Val Loss: 0.6175, Val Acc: 93.33%, Test Loss: 0.6214, Test Acc: 93.33%\n",
      "Epoch [272/1500] - Train Loss: 0.5800, Train Acc: 97.60%, Val Loss: 0.6158, Val Acc: 93.33%, Test Loss: 0.6215, Test Acc: 93.33%\n",
      "Epoch [273/1500] - Train Loss: 0.5826, Train Acc: 96.88%, Val Loss: 0.6206, Val Acc: 92.50%, Test Loss: 0.6175, Test Acc: 93.33%\n",
      "Epoch [274/1500] - Train Loss: 0.5811, Train Acc: 97.50%, Val Loss: 0.6176, Val Acc: 92.50%, Test Loss: 0.6188, Test Acc: 93.33%\n",
      "Epoch [275/1500] - Train Loss: 0.5821, Train Acc: 97.29%, Val Loss: 0.6077, Val Acc: 94.17%, Test Loss: 0.6257, Test Acc: 91.67%\n",
      "Epoch [276/1500] - Train Loss: 0.5803, Train Acc: 97.50%, Val Loss: 0.6064, Val Acc: 94.17%, Test Loss: 0.6236, Test Acc: 92.50%\n",
      "Epoch [277/1500] - Train Loss: 0.5828, Train Acc: 96.98%, Val Loss: 0.6113, Val Acc: 94.17%, Test Loss: 0.6243, Test Acc: 91.67%\n",
      "Epoch [278/1500] - Train Loss: 0.5806, Train Acc: 97.50%, Val Loss: 0.6108, Val Acc: 94.17%, Test Loss: 0.6251, Test Acc: 91.67%\n",
      "Epoch [279/1500] - Train Loss: 0.5826, Train Acc: 97.08%, Val Loss: 0.6112, Val Acc: 93.33%, Test Loss: 0.6257, Test Acc: 91.67%\n",
      "Epoch [280/1500] - Train Loss: 0.5797, Train Acc: 97.40%, Val Loss: 0.6130, Val Acc: 92.50%, Test Loss: 0.6187, Test Acc: 93.33%\n",
      "Epoch [281/1500] - Train Loss: 0.5822, Train Acc: 96.77%, Val Loss: 0.6150, Val Acc: 93.33%, Test Loss: 0.6141, Test Acc: 93.33%\n",
      "Epoch [282/1500] - Train Loss: 0.5803, Train Acc: 97.29%, Val Loss: 0.6149, Val Acc: 92.50%, Test Loss: 0.6164, Test Acc: 93.33%\n",
      "Epoch [283/1500] - Train Loss: 0.5806, Train Acc: 97.29%, Val Loss: 0.6096, Val Acc: 93.33%, Test Loss: 0.6308, Test Acc: 91.67%\n",
      "Epoch [284/1500] - Train Loss: 0.5820, Train Acc: 97.19%, Val Loss: 0.6107, Val Acc: 93.33%, Test Loss: 0.6214, Test Acc: 92.50%\n",
      "Epoch [285/1500] - Train Loss: 0.5784, Train Acc: 97.50%, Val Loss: 0.6055, Val Acc: 94.17%, Test Loss: 0.6305, Test Acc: 91.67%\n",
      "Epoch [286/1500] - Train Loss: 0.5794, Train Acc: 97.40%, Val Loss: 0.6093, Val Acc: 94.17%, Test Loss: 0.6225, Test Acc: 92.50%\n",
      "Epoch [287/1500] - Train Loss: 0.5801, Train Acc: 97.50%, Val Loss: 0.6112, Val Acc: 93.33%, Test Loss: 0.6218, Test Acc: 92.50%\n",
      "Epoch [288/1500] - Train Loss: 0.5790, Train Acc: 97.71%, Val Loss: 0.6117, Val Acc: 93.33%, Test Loss: 0.6138, Test Acc: 94.17%\n",
      "Epoch [289/1500] - Train Loss: 0.5809, Train Acc: 97.29%, Val Loss: 0.6067, Val Acc: 94.17%, Test Loss: 0.6276, Test Acc: 91.67%\n",
      "Epoch [290/1500] - Train Loss: 0.5798, Train Acc: 97.40%, Val Loss: 0.6084, Val Acc: 94.17%, Test Loss: 0.6208, Test Acc: 92.50%\n",
      "Epoch [291/1500] - Train Loss: 0.5769, Train Acc: 97.71%, Val Loss: 0.6084, Val Acc: 93.33%, Test Loss: 0.6149, Test Acc: 94.17%\n",
      "Epoch [292/1500] - Train Loss: 0.5807, Train Acc: 97.08%, Val Loss: 0.6073, Val Acc: 94.17%, Test Loss: 0.6302, Test Acc: 91.67%\n",
      "Epoch [293/1500] - Train Loss: 0.5810, Train Acc: 97.29%, Val Loss: 0.6088, Val Acc: 94.17%, Test Loss: 0.6191, Test Acc: 92.50%\n",
      "Epoch [294/1500] - Train Loss: 0.5772, Train Acc: 97.71%, Val Loss: 0.6081, Val Acc: 94.17%, Test Loss: 0.6174, Test Acc: 93.33%\n",
      "Epoch [295/1500] - Train Loss: 0.5782, Train Acc: 97.71%, Val Loss: 0.6108, Val Acc: 93.33%, Test Loss: 0.6164, Test Acc: 92.50%\n",
      "Epoch [296/1500] - Train Loss: 0.5778, Train Acc: 97.60%, Val Loss: 0.6091, Val Acc: 93.33%, Test Loss: 0.6133, Test Acc: 94.17%\n",
      "Epoch [297/1500] - Train Loss: 0.5773, Train Acc: 97.60%, Val Loss: 0.6061, Val Acc: 94.17%, Test Loss: 0.6151, Test Acc: 94.17%\n",
      "Epoch [298/1500] - Train Loss: 0.5780, Train Acc: 97.60%, Val Loss: 0.6021, Val Acc: 95.00%, Test Loss: 0.6298, Test Acc: 91.67%\n",
      "Epoch [299/1500] - Train Loss: 0.5798, Train Acc: 97.40%, Val Loss: 0.6098, Val Acc: 93.33%, Test Loss: 0.6132, Test Acc: 94.17%\n",
      "Epoch [300/1500] - Train Loss: 0.5776, Train Acc: 97.50%, Val Loss: 0.6085, Val Acc: 94.17%, Test Loss: 0.6235, Test Acc: 92.50%\n",
      "Epoch [301/1500] - Train Loss: 0.5797, Train Acc: 97.29%, Val Loss: 0.6083, Val Acc: 94.17%, Test Loss: 0.6175, Test Acc: 93.33%\n",
      "Epoch [302/1500] - Train Loss: 0.5773, Train Acc: 97.71%, Val Loss: 0.6084, Val Acc: 94.17%, Test Loss: 0.6211, Test Acc: 92.50%\n",
      "Epoch [303/1500] - Train Loss: 0.5788, Train Acc: 97.40%, Val Loss: 0.6079, Val Acc: 94.17%, Test Loss: 0.6185, Test Acc: 93.33%\n",
      "Epoch [304/1500] - Train Loss: 0.5768, Train Acc: 97.60%, Val Loss: 0.6088, Val Acc: 93.33%, Test Loss: 0.6134, Test Acc: 94.17%\n",
      "Epoch [305/1500] - Train Loss: 0.5783, Train Acc: 97.60%, Val Loss: 0.6123, Val Acc: 93.33%, Test Loss: 0.6088, Test Acc: 94.17%\n",
      "Epoch [306/1500] - Train Loss: 0.5801, Train Acc: 97.40%, Val Loss: 0.6124, Val Acc: 93.33%, Test Loss: 0.6096, Test Acc: 95.00%\n",
      "Epoch [307/1500] - Train Loss: 0.5772, Train Acc: 97.81%, Val Loss: 0.6076, Val Acc: 95.00%, Test Loss: 0.6119, Test Acc: 94.17%\n",
      "Epoch [308/1500] - Train Loss: 0.5759, Train Acc: 97.71%, Val Loss: 0.6068, Val Acc: 94.17%, Test Loss: 0.6181, Test Acc: 93.33%\n",
      "Epoch [309/1500] - Train Loss: 0.5776, Train Acc: 97.40%, Val Loss: 0.6075, Val Acc: 94.17%, Test Loss: 0.6136, Test Acc: 93.33%\n",
      "Epoch [310/1500] - Train Loss: 0.5762, Train Acc: 97.71%, Val Loss: 0.6108, Val Acc: 93.33%, Test Loss: 0.6168, Test Acc: 93.33%\n",
      "Epoch [311/1500] - Train Loss: 0.5759, Train Acc: 97.92%, Val Loss: 0.6061, Val Acc: 94.17%, Test Loss: 0.6127, Test Acc: 94.17%\n",
      "Epoch [312/1500] - Train Loss: 0.5764, Train Acc: 97.81%, Val Loss: 0.6063, Val Acc: 95.00%, Test Loss: 0.6123, Test Acc: 94.17%\n",
      "Epoch [313/1500] - Train Loss: 0.5764, Train Acc: 97.60%, Val Loss: 0.6066, Val Acc: 94.17%, Test Loss: 0.6204, Test Acc: 93.33%\n",
      "Epoch [314/1500] - Train Loss: 0.5775, Train Acc: 97.60%, Val Loss: 0.6085, Val Acc: 93.33%, Test Loss: 0.6142, Test Acc: 93.33%\n",
      "Epoch [315/1500] - Train Loss: 0.5756, Train Acc: 97.92%, Val Loss: 0.6031, Val Acc: 94.17%, Test Loss: 0.6236, Test Acc: 92.50%\n",
      "Epoch [316/1500] - Train Loss: 0.5781, Train Acc: 97.50%, Val Loss: 0.6060, Val Acc: 94.17%, Test Loss: 0.6218, Test Acc: 92.50%\n",
      "Epoch [317/1500] - Train Loss: 0.5779, Train Acc: 97.60%, Val Loss: 0.6048, Val Acc: 94.17%, Test Loss: 0.6140, Test Acc: 93.33%\n",
      "Epoch [318/1500] - Train Loss: 0.5743, Train Acc: 97.92%, Val Loss: 0.6058, Val Acc: 94.17%, Test Loss: 0.6169, Test Acc: 93.33%\n",
      "Epoch [319/1500] - Train Loss: 0.5758, Train Acc: 97.71%, Val Loss: 0.6096, Val Acc: 93.33%, Test Loss: 0.6129, Test Acc: 93.33%\n",
      "Epoch [320/1500] - Train Loss: 0.5751, Train Acc: 97.81%, Val Loss: 0.6094, Val Acc: 93.33%, Test Loss: 0.6185, Test Acc: 93.33%\n",
      "Epoch [321/1500] - Train Loss: 0.5766, Train Acc: 97.71%, Val Loss: 0.6068, Val Acc: 94.17%, Test Loss: 0.6181, Test Acc: 93.33%\n",
      "Epoch [322/1500] - Train Loss: 0.5743, Train Acc: 98.02%, Val Loss: 0.6108, Val Acc: 93.33%, Test Loss: 0.6130, Test Acc: 93.33%\n",
      "Epoch [323/1500] - Train Loss: 0.5745, Train Acc: 97.81%, Val Loss: 0.6066, Val Acc: 94.17%, Test Loss: 0.6260, Test Acc: 91.67%\n",
      "Epoch [324/1500] - Train Loss: 0.5764, Train Acc: 97.71%, Val Loss: 0.6084, Val Acc: 94.17%, Test Loss: 0.6106, Test Acc: 94.17%\n",
      "Epoch [325/1500] - Train Loss: 0.5763, Train Acc: 97.71%, Val Loss: 0.6104, Val Acc: 93.33%, Test Loss: 0.6186, Test Acc: 93.33%\n",
      "Epoch [326/1500] - Train Loss: 0.5764, Train Acc: 97.71%, Val Loss: 0.6105, Val Acc: 93.33%, Test Loss: 0.6141, Test Acc: 93.33%\n",
      "Epoch [327/1500] - Train Loss: 0.5757, Train Acc: 97.81%, Val Loss: 0.6055, Val Acc: 94.17%, Test Loss: 0.6193, Test Acc: 93.33%\n",
      "Epoch [328/1500] - Train Loss: 0.5756, Train Acc: 97.71%, Val Loss: 0.6101, Val Acc: 93.33%, Test Loss: 0.6089, Test Acc: 94.17%\n",
      "Epoch [329/1500] - Train Loss: 0.5761, Train Acc: 97.60%, Val Loss: 0.6096, Val Acc: 93.33%, Test Loss: 0.6183, Test Acc: 93.33%\n",
      "Epoch [330/1500] - Train Loss: 0.5757, Train Acc: 97.81%, Val Loss: 0.6106, Val Acc: 93.33%, Test Loss: 0.6155, Test Acc: 93.33%\n",
      "Epoch [331/1500] - Train Loss: 0.5753, Train Acc: 97.81%, Val Loss: 0.6075, Val Acc: 94.17%, Test Loss: 0.6200, Test Acc: 93.33%\n",
      "Epoch [332/1500] - Train Loss: 0.5767, Train Acc: 97.81%, Val Loss: 0.6088, Val Acc: 93.33%, Test Loss: 0.6140, Test Acc: 93.33%\n",
      "Epoch [333/1500] - Train Loss: 0.5745, Train Acc: 97.81%, Val Loss: 0.6076, Val Acc: 94.17%, Test Loss: 0.6109, Test Acc: 94.17%\n",
      "Epoch [334/1500] - Train Loss: 0.5746, Train Acc: 97.92%, Val Loss: 0.6089, Val Acc: 93.33%, Test Loss: 0.6124, Test Acc: 93.33%\n",
      "Epoch [335/1500] - Train Loss: 0.5764, Train Acc: 97.50%, Val Loss: 0.6091, Val Acc: 94.17%, Test Loss: 0.6114, Test Acc: 94.17%\n",
      "Epoch [336/1500] - Train Loss: 0.5754, Train Acc: 97.71%, Val Loss: 0.6061, Val Acc: 94.17%, Test Loss: 0.6179, Test Acc: 93.33%\n",
      "Epoch [337/1500] - Train Loss: 0.5772, Train Acc: 97.60%, Val Loss: 0.6064, Val Acc: 93.33%, Test Loss: 0.6148, Test Acc: 93.33%\n",
      "Epoch [338/1500] - Train Loss: 0.5746, Train Acc: 97.92%, Val Loss: 0.6084, Val Acc: 94.17%, Test Loss: 0.6118, Test Acc: 93.33%\n",
      "Epoch [339/1500] - Train Loss: 0.5767, Train Acc: 97.71%, Val Loss: 0.6077, Val Acc: 94.17%, Test Loss: 0.6082, Test Acc: 94.17%\n",
      "Epoch [340/1500] - Train Loss: 0.5742, Train Acc: 98.02%, Val Loss: 0.6088, Val Acc: 93.33%, Test Loss: 0.6198, Test Acc: 93.33%\n",
      "Epoch [341/1500] - Train Loss: 0.5745, Train Acc: 97.81%, Val Loss: 0.6075, Val Acc: 94.17%, Test Loss: 0.6085, Test Acc: 95.00%\n",
      "Epoch [342/1500] - Train Loss: 0.5737, Train Acc: 97.92%, Val Loss: 0.6056, Val Acc: 94.17%, Test Loss: 0.6129, Test Acc: 93.33%\n",
      "Epoch [343/1500] - Train Loss: 0.5764, Train Acc: 97.71%, Val Loss: 0.6079, Val Acc: 94.17%, Test Loss: 0.6207, Test Acc: 93.33%\n",
      "Epoch [344/1500] - Train Loss: 0.5739, Train Acc: 97.81%, Val Loss: 0.6077, Val Acc: 94.17%, Test Loss: 0.6097, Test Acc: 94.17%\n",
      "Epoch [345/1500] - Train Loss: 0.5745, Train Acc: 98.02%, Val Loss: 0.6072, Val Acc: 94.17%, Test Loss: 0.6232, Test Acc: 92.50%\n",
      "Epoch [346/1500] - Train Loss: 0.5758, Train Acc: 97.71%, Val Loss: 0.6072, Val Acc: 93.33%, Test Loss: 0.6143, Test Acc: 93.33%\n",
      "Epoch [347/1500] - Train Loss: 0.5744, Train Acc: 97.81%, Val Loss: 0.6071, Val Acc: 94.17%, Test Loss: 0.6104, Test Acc: 94.17%\n",
      "Epoch [348/1500] - Train Loss: 0.5734, Train Acc: 97.92%, Val Loss: 0.6072, Val Acc: 93.33%, Test Loss: 0.6135, Test Acc: 93.33%\n",
      "Epoch [349/1500] - Train Loss: 0.5727, Train Acc: 97.92%, Val Loss: 0.6101, Val Acc: 93.33%, Test Loss: 0.6158, Test Acc: 93.33%\n",
      "Epoch [350/1500] - Train Loss: 0.5743, Train Acc: 97.81%, Val Loss: 0.6109, Val Acc: 93.33%, Test Loss: 0.6160, Test Acc: 93.33%\n",
      "Epoch [351/1500] - Train Loss: 0.5743, Train Acc: 97.81%, Val Loss: 0.6056, Val Acc: 94.17%, Test Loss: 0.6127, Test Acc: 93.33%\n",
      "Epoch [352/1500] - Train Loss: 0.5737, Train Acc: 98.02%, Val Loss: 0.6085, Val Acc: 94.17%, Test Loss: 0.6128, Test Acc: 93.33%\n",
      "Epoch [353/1500] - Train Loss: 0.5732, Train Acc: 98.12%, Val Loss: 0.6096, Val Acc: 93.33%, Test Loss: 0.6095, Test Acc: 94.17%\n",
      "Epoch [354/1500] - Train Loss: 0.5764, Train Acc: 97.71%, Val Loss: 0.6080, Val Acc: 94.17%, Test Loss: 0.6076, Test Acc: 94.17%\n",
      "Epoch [355/1500] - Train Loss: 0.5744, Train Acc: 97.92%, Val Loss: 0.6086, Val Acc: 93.33%, Test Loss: 0.6140, Test Acc: 93.33%\n",
      "Epoch [356/1500] - Train Loss: 0.5743, Train Acc: 97.92%, Val Loss: 0.6068, Val Acc: 94.17%, Test Loss: 0.6114, Test Acc: 94.17%\n",
      "Epoch [357/1500] - Train Loss: 0.5712, Train Acc: 98.23%, Val Loss: 0.6072, Val Acc: 94.17%, Test Loss: 0.6097, Test Acc: 94.17%\n",
      "Epoch [358/1500] - Train Loss: 0.5743, Train Acc: 97.92%, Val Loss: 0.6072, Val Acc: 94.17%, Test Loss: 0.6121, Test Acc: 93.33%\n",
      "Epoch [359/1500] - Train Loss: 0.5723, Train Acc: 98.12%, Val Loss: 0.6073, Val Acc: 93.33%, Test Loss: 0.6144, Test Acc: 93.33%\n",
      "Epoch [360/1500] - Train Loss: 0.5726, Train Acc: 98.02%, Val Loss: 0.6069, Val Acc: 94.17%, Test Loss: 0.6112, Test Acc: 94.17%\n",
      "Epoch [361/1500] - Train Loss: 0.5723, Train Acc: 98.23%, Val Loss: 0.6077, Val Acc: 93.33%, Test Loss: 0.6130, Test Acc: 93.33%\n",
      "Epoch [362/1500] - Train Loss: 0.5731, Train Acc: 98.12%, Val Loss: 0.6063, Val Acc: 94.17%, Test Loss: 0.6129, Test Acc: 93.33%\n",
      "Epoch [363/1500] - Train Loss: 0.5734, Train Acc: 98.02%, Val Loss: 0.6077, Val Acc: 94.17%, Test Loss: 0.6135, Test Acc: 93.33%\n",
      "Epoch [364/1500] - Train Loss: 0.5724, Train Acc: 98.12%, Val Loss: 0.6100, Val Acc: 93.33%, Test Loss: 0.6168, Test Acc: 93.33%\n",
      "Epoch [365/1500] - Train Loss: 0.5734, Train Acc: 98.02%, Val Loss: 0.6056, Val Acc: 94.17%, Test Loss: 0.6242, Test Acc: 91.67%\n",
      "Epoch [366/1500] - Train Loss: 0.5764, Train Acc: 97.50%, Val Loss: 0.6073, Val Acc: 93.33%, Test Loss: 0.6147, Test Acc: 93.33%\n",
      "Epoch [367/1500] - Train Loss: 0.5732, Train Acc: 98.02%, Val Loss: 0.6069, Val Acc: 94.17%, Test Loss: 0.6063, Test Acc: 94.17%\n",
      "Epoch [368/1500] - Train Loss: 0.5740, Train Acc: 98.02%, Val Loss: 0.6062, Val Acc: 94.17%, Test Loss: 0.6104, Test Acc: 94.17%\n",
      "Epoch [369/1500] - Train Loss: 0.5719, Train Acc: 98.23%, Val Loss: 0.6055, Val Acc: 95.00%, Test Loss: 0.6063, Test Acc: 95.00%\n",
      "Epoch [370/1500] - Train Loss: 0.5713, Train Acc: 98.33%, Val Loss: 0.6080, Val Acc: 93.33%, Test Loss: 0.6145, Test Acc: 93.33%\n",
      "Epoch [371/1500] - Train Loss: 0.5731, Train Acc: 98.12%, Val Loss: 0.6083, Val Acc: 93.33%, Test Loss: 0.6145, Test Acc: 93.33%\n",
      "Epoch [372/1500] - Train Loss: 0.5721, Train Acc: 98.23%, Val Loss: 0.6066, Val Acc: 94.17%, Test Loss: 0.6126, Test Acc: 93.33%\n",
      "Epoch [373/1500] - Train Loss: 0.5720, Train Acc: 98.12%, Val Loss: 0.6061, Val Acc: 94.17%, Test Loss: 0.6077, Test Acc: 94.17%\n",
      "Epoch [374/1500] - Train Loss: 0.5716, Train Acc: 98.12%, Val Loss: 0.6089, Val Acc: 93.33%, Test Loss: 0.6156, Test Acc: 93.33%\n",
      "Epoch [375/1500] - Train Loss: 0.5706, Train Acc: 98.23%, Val Loss: 0.6071, Val Acc: 94.17%, Test Loss: 0.6129, Test Acc: 93.33%\n",
      "Epoch [376/1500] - Train Loss: 0.5710, Train Acc: 98.33%, Val Loss: 0.6074, Val Acc: 94.17%, Test Loss: 0.6137, Test Acc: 93.33%\n",
      "Epoch [377/1500] - Train Loss: 0.5711, Train Acc: 98.23%, Val Loss: 0.6063, Val Acc: 94.17%, Test Loss: 0.6118, Test Acc: 93.33%\n",
      "Epoch [378/1500] - Train Loss: 0.5704, Train Acc: 98.23%, Val Loss: 0.6047, Val Acc: 95.00%, Test Loss: 0.6085, Test Acc: 94.17%\n",
      "Epoch [379/1500] - Train Loss: 0.5741, Train Acc: 98.02%, Val Loss: 0.6041, Val Acc: 95.00%, Test Loss: 0.6089, Test Acc: 94.17%\n",
      "Epoch [380/1500] - Train Loss: 0.5712, Train Acc: 98.23%, Val Loss: 0.6078, Val Acc: 93.33%, Test Loss: 0.6148, Test Acc: 93.33%\n",
      "Epoch [381/1500] - Train Loss: 0.5710, Train Acc: 98.23%, Val Loss: 0.6068, Val Acc: 94.17%, Test Loss: 0.6139, Test Acc: 93.33%\n",
      "Epoch [382/1500] - Train Loss: 0.5712, Train Acc: 98.23%, Val Loss: 0.6066, Val Acc: 94.17%, Test Loss: 0.6080, Test Acc: 94.17%\n",
      "Epoch [383/1500] - Train Loss: 0.5711, Train Acc: 98.12%, Val Loss: 0.6060, Val Acc: 94.17%, Test Loss: 0.6098, Test Acc: 94.17%\n",
      "Epoch [384/1500] - Train Loss: 0.5705, Train Acc: 98.23%, Val Loss: 0.6060, Val Acc: 94.17%, Test Loss: 0.6097, Test Acc: 94.17%\n",
      "Epoch [385/1500] - Train Loss: 0.5720, Train Acc: 98.23%, Val Loss: 0.6050, Val Acc: 95.00%, Test Loss: 0.6076, Test Acc: 94.17%\n",
      "Epoch [386/1500] - Train Loss: 0.5708, Train Acc: 98.12%, Val Loss: 0.6094, Val Acc: 93.33%, Test Loss: 0.6172, Test Acc: 93.33%\n",
      "Epoch [387/1500] - Train Loss: 0.5715, Train Acc: 98.12%, Val Loss: 0.6072, Val Acc: 94.17%, Test Loss: 0.6072, Test Acc: 94.17%\n",
      "Epoch [388/1500] - Train Loss: 0.5716, Train Acc: 98.12%, Val Loss: 0.6056, Val Acc: 94.17%, Test Loss: 0.6101, Test Acc: 94.17%\n",
      "Epoch [389/1500] - Train Loss: 0.5701, Train Acc: 98.33%, Val Loss: 0.6063, Val Acc: 94.17%, Test Loss: 0.6110, Test Acc: 94.17%\n",
      "Epoch [390/1500] - Train Loss: 0.5720, Train Acc: 98.02%, Val Loss: 0.6056, Val Acc: 94.17%, Test Loss: 0.6100, Test Acc: 94.17%\n",
      "Epoch [391/1500] - Train Loss: 0.5707, Train Acc: 98.23%, Val Loss: 0.6064, Val Acc: 94.17%, Test Loss: 0.6104, Test Acc: 94.17%\n",
      "Epoch [392/1500] - Train Loss: 0.5711, Train Acc: 98.23%, Val Loss: 0.6075, Val Acc: 94.17%, Test Loss: 0.6143, Test Acc: 93.33%\n",
      "Epoch [393/1500] - Train Loss: 0.5703, Train Acc: 98.23%, Val Loss: 0.6047, Val Acc: 94.17%, Test Loss: 0.6110, Test Acc: 94.17%\n",
      "Epoch [394/1500] - Train Loss: 0.5716, Train Acc: 98.02%, Val Loss: 0.6048, Val Acc: 94.17%, Test Loss: 0.6100, Test Acc: 94.17%\n",
      "Epoch [395/1500] - Train Loss: 0.5708, Train Acc: 98.23%, Val Loss: 0.6073, Val Acc: 94.17%, Test Loss: 0.6129, Test Acc: 93.33%\n",
      "Epoch [396/1500] - Train Loss: 0.5707, Train Acc: 98.23%, Val Loss: 0.6069, Val Acc: 93.33%, Test Loss: 0.6158, Test Acc: 93.33%\n",
      "Epoch [397/1500] - Train Loss: 0.5698, Train Acc: 98.23%, Val Loss: 0.6097, Val Acc: 93.33%, Test Loss: 0.6061, Test Acc: 94.17%\n",
      "Epoch [398/1500] - Train Loss: 0.5714, Train Acc: 98.12%, Val Loss: 0.6080, Val Acc: 93.33%, Test Loss: 0.6148, Test Acc: 93.33%\n",
      "Early stopping triggered at epoch 398\n",
      "Training completed. Best model saved at /home/user/torch_shrimp/until-tools/mod/k-mer/es400/batch400-3/best_model.pth\n",
      "Latest model saved at /home/user/torch_shrimp/until-tools/mod/k-mer/es400/batch400-3/latest_model.pth\n",
      "/tmp/ipykernel_150689/2446900486.py:203: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load(best_model_path))\n",
      "Train Loss: 0.5714, Train Acc: 98.12%, Val Loss: 0.6080, Val Acc: 93.33%, Test Loss: 0.6148, Test Acc: 91.67%\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9474    0.9231    0.9351        39\n",
      "           1     0.9000    1.0000    0.9474        45\n",
      "           2     0.9062    0.8056    0.8529        36\n",
      "\n",
      "    accuracy                         0.9167       120\n",
      "   macro avg     0.9179    0.9095    0.9118       120\n",
      "weighted avg     0.9173    0.9167    0.9150       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#version 7\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set log directory\n",
    "log_dir = \"/home/user/torch_shrimp/until-tools/mod/k-mer/es400/batch400-3\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Ensure the log filename is the same as the directory name\n",
    "log_file = os.path.join(log_dir, f\"{os.path.basename(log_dir)}.txt\")\n",
    "\n",
    "# Remove previous logging handlers to prevent duplicate logs\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# Function to log hyperparameters\n",
    "def log_hyperparameters(hyperparams):\n",
    "    logging.info(\"Hyperparameters:\")\n",
    "    logging.info(json.dumps(hyperparams, indent=4))\n",
    "\n",
    "# Custom Dataset for k-mer frequency data\n",
    "class KmerFrequencyDataset(Dataset):\n",
    "    def __init__(self, kmer_score_file):\n",
    "        self.data = pd.read_csv(kmer_score_file)\n",
    "        self.features = self.data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "        self.labels = self.data['status'].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True, dropout=0.5)\n",
    "        self.fc1 = nn.Linear(hidden_dim2, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"embedding_dim\": 128,\n",
    "    \"hidden_dim1\": 64,\n",
    "    \"hidden_dim2\": 32,\n",
    "    \"dense_units\": 64,\n",
    "    \"num_classes\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"num_epochs\": 1500,\n",
    "    \"batch_size\": 32,\n",
    "    \"train_size\": 0.8,\n",
    "    \"val_size\": 0.1,\n",
    "    \"file_path\": \"/home/user/torch_shrimp/until-tools/mod/k-mer/dataset/train400.csv\",\n",
    "    \"patience\": 100\n",
    "}\n",
    "log_hyperparameters(hyperparams)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = KmerFrequencyDataset(hyperparams[\"file_path\"])\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(hyperparams[\"train_size\"] * dataset_size)\n",
    "val_size = int(hyperparams[\"val_size\"] * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparams[\"batch_size\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams[\"batch_size\"])\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "input_dim = dataset.features.shape[1]\n",
    "model = LSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "    hidden_dim1=hyperparams[\"hidden_dim1\"],\n",
    "    hidden_dim2=hyperparams[\"hidden_dim2\"],\n",
    "    dense_units=hyperparams[\"dense_units\"],\n",
    "    num_classes=hyperparams[\"num_classes\"]\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "\n",
    "# Paths for saving models\n",
    "best_model_path = os.path.join(log_dir, \"best_model.pth\")\n",
    "latest_model_path = os.path.join(log_dir, \"latest_model.pth\")\n",
    "\n",
    "# Training and validation loop\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = hyperparams[\"patience\"]\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(hyperparams[\"num_epochs\"]):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += y_batch.size(0)\n",
    "            val_correct += (predicted == y_batch).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    # Testing phase\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            test_total += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "    logging.info(f\"Epoch [{epoch + 1}/{hyperparams['num_epochs']}] - \"\n",
    "                 f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                 f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, \"\n",
    "                 f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Save latest model\n",
    "    torch.save(model.state_dict(), latest_model_path)\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            logging.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "logging.info(f\"Training completed. Best model saved at {best_model_path}\")\n",
    "logging.info(f\"Latest model saved at {latest_model_path}\")\n",
    "\n",
    "# Load the best model for evaluation\n",
    "best_model = LSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "    hidden_dim1=hyperparams[\"hidden_dim1\"],\n",
    "    hidden_dim2=hyperparams[\"hidden_dim2\"],\n",
    "    dense_units=hyperparams[\"dense_units\"],\n",
    "    num_classes=hyperparams[\"num_classes\"]\n",
    ")\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.eval()\n",
    "\n",
    "# Evaluation on test set with F1-score\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(torch.float32)\n",
    "        output = best_model(X_batch)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "        test_total += y_batch.size(0)\n",
    "        test_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "f1_report = classification_report(all_labels, all_preds, digits=4)\n",
    "\n",
    "logging.info(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "                 f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, \"\n",
    "                 f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
    "logging.info(\"Final Classification Report:\")\n",
    "logging.info(f1_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece9356a-bbf7-41ab-8dd7-5679d5eec4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Custom Dataset for k-mer frequency data\n",
    "class KmerFrequencyDataset(Dataset):\n",
    "    def __init__(self, kmer_score_file):\n",
    "        # Load scoring file\n",
    "        self.data = pd.read_csv(kmer_score_file)\n",
    "\n",
    "        # Extract features and labels\n",
    "        self.features = self.data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "        self.labels = self.data['status'].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Define LSTM model with embedding layer\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True, dropout=0.5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim2, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_dim) -> (batch_size, embedding_dim)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension for LSTM\n",
    "\n",
    "        # LSTM layers\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "\n",
    "        # Extract the last output of the LSTM\n",
    "        x = x[:, -1, :]  # Use the last hidden state\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)  # Output probabilities\n",
    "\n",
    "# Hyperparameters and file paths\n",
    "embedding_dim = 128\n",
    "hidden_dim1 = 64\n",
    "hidden_dim2 = 32\n",
    "dense_units = 64\n",
    "num_classes = 3\n",
    "batch_size = 32\n",
    "\n",
    "kmer_score_file = '/home/user/torch_shrimp/until-tools/mod/k-mer/test3.csv'\n",
    "model_path = '/home/user/torch_shrimp/until-tools/mod/k-mer/updated_model_1.pth'  # Path to the saved model file\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = KmerFrequencyDataset(kmer_score_file)\n",
    "dataset_size = len(dataset)\n",
    "_, _, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, \n",
    "    [int(0.8 * dataset_size), int(0.1 * dataset_size), dataset_size - int(0.8 * dataset_size) - int(0.1 * dataset_size)]\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = dataset.features.shape[1]  # Automatically adjust to input feature size\n",
    "model = LSTMModel(input_dim, embedding_dim, hidden_dim1, hidden_dim2, dense_units, num_classes)\n",
    "\n",
    "# Load model weights\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation on test set\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(torch.float32)\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "        test_total += y_batch.size(0)\n",
    "        test_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60049342-1fec-4359-9014-b0aa79155f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/torch_shrimp/lib/python3.10/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/tmp/ipykernel_150689/4053450289.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average True Prediction Probability: 0.9942\n",
      "Number of True Predictions: 284 (94.67%)\n",
      "Number of False Predictions: 16 (5.33%)\n",
      "\n",
      "Predictions for test samples:\n",
      "Sample 1:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [5.973054430796765e-05, 2.279859018017305e-06, 0.9999380111694336]\n",
      "Sample 2:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [7.436559826601297e-06, 3.3737815101630986e-05, 0.999958872795105]\n",
      "Sample 3:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.1116862879134715e-06, 3.990975585566048e-07, 0.9999984502792358]\n",
      "Sample 4:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [2.353409399802331e-06, 7.926922194201325e-07, 0.9999969005584717]\n",
      "Sample 5:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [3.107519432887784e-06, 1.016458440972201e-06, 0.9999958276748657]\n",
      "Sample 6:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.2104836059734225e-05, 6.997918831075367e-07, 0.9999872446060181]\n",
      "Sample 7:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [2.5508554699626984e-06, 5.0986200221814215e-06, 0.9999923706054688]\n",
      "Sample 8:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.168979906651657e-06, 7.157842105698364e-07, 0.9999980926513672]\n",
      "Sample 9:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [3.377172561158659e-06, 5.693660227734654e-07, 0.9999960660934448]\n",
      "Sample 10:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.00013199372915551066, 0.059613000601530075, 0.9402549862861633]\n",
      "Sample 11:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [6.103364739828976e-06, 5.7050815485126805e-06, 0.9999881982803345]\n",
      "Sample 12:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [6.0491579461086076e-06, 5.548137096411665e-07, 0.9999934434890747]\n",
      "Sample 13:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.401986173732439e-05, 0.000193964850041084, 0.9997920393943787]\n",
      "Sample 14:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.6604194570390973e-06, 0.9995203018188477, 0.00047808271483518183]\n",
      "Sample 15:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [3.334936081955675e-06, 2.1266423573251814e-06, 0.9999945163726807]\n",
      "Sample 16:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [3.18044658342842e-05, 7.125796400941908e-05, 0.9998968839645386]\n",
      "Sample 17:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999983310699463, 1.3978344037823831e-09, 1.6981673525151564e-06]\n",
      "Sample 18:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [5.0045791795128025e-06, 1.3089676031086128e-05, 0.9999818801879883]\n",
      "Sample 19:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [0.00048234494170174, 0.9047709107398987, 0.09474672377109528]\n",
      "Sample 20:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [9.787581802811474e-07, 3.943155775232299e-07, 0.9999985694885254]\n",
      "Sample 21:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.00010799390292959288, 1.0711951290431898e-05, 0.9998812675476074]\n",
      "Sample 22:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [4.586694103636546e-06, 9.022684025694616e-06, 0.9999864101409912]\n",
      "Sample 23:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [9.150690857495647e-06, 0.000150578809552826, 0.999840259552002]\n",
      "Sample 24:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.00014679640298709273, 0.3810531497001648, 0.6188000440597534]\n",
      "Sample 25:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [2.6242717012792127e-06, 3.602739582220238e-07, 0.9999970197677612]\n",
      "Sample 26:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [2.4280216166516766e-05, 2.313431195943849e-06, 0.9999734163284302]\n",
      "Sample 27:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [5.2616846915043425e-06, 4.002873174613342e-05, 0.9999547004699707]\n",
      "Sample 28:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [3.559296237654053e-05, 0.0005461222026497126, 0.9994182586669922]\n",
      "Sample 29:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.4140860912448261e-05, 2.222071088908706e-06, 0.9999836683273315]\n",
      "Sample 30:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.8820284140019794e-06, 1.6293781754939118e-06, 0.999996542930603]\n",
      "Sample 31:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [4.595574864652008e-06, 2.306153874087613e-05, 0.9999723434448242]\n",
      "Sample 32:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9994493126869202, 3.723306463143672e-07, 0.0005503213033080101]\n",
      "Sample 33:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9031380414962769, 3.5008972645300673e-06, 0.09685838222503662]\n",
      "Sample 34:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.6543937135793385e-06, 1.5697175967943622e-06, 0.9999967813491821]\n",
      "Sample 35:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [4.7373774577863514e-05, 0.03996952623128891, 0.9599831104278564]\n",
      "Sample 36:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.7262693745578872e-06, 2.514892685212544e-06, 0.9999957084655762]\n",
      "Sample 37:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [7.593447435283451e-07, 3.1898784413897374e-07, 0.999998927116394]\n",
      "Sample 38:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [8.73348290042486e-06, 0.00011751373676816002, 0.9998737573623657]\n",
      "Sample 39:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [8.490387699566782e-06, 5.093379968457157e-06, 0.9999864101409912]\n",
      "Sample 40:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [9.351937478641048e-05, 0.3321816921234131, 0.6677247881889343]\n",
      "Sample 41:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.2471205081965309e-06, 8.131692084134556e-07, 0.9999979734420776]\n",
      "Sample 42:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [7.348571216425626e-06, 7.629195806657663e-06, 0.9999849796295166]\n",
      "Sample 43:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.2220498319948092e-06, 9.64556875260314e-07, 0.9999978542327881]\n",
      "Sample 44:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.0001480128412367776, 5.415453415480442e-06, 0.9998465776443481]\n",
      "Sample 45:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [3.74169867427554e-05, 0.0004047024413011968, 0.9995579123497009]\n",
      "Sample 46:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.4176732747728238e-06, 4.3504860514076427e-07, 0.9999980926513672]\n",
      "Sample 47:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.190785496874014e-06, 3.445709637617256e-07, 0.9999984502792358]\n",
      "Sample 48:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [5.9379467529652175e-06, 5.683792778654606e-07, 0.9999934434890747]\n",
      "Sample 49:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.156062808149727e-05, 2.5615363483666442e-06, 0.999985933303833]\n",
      "Sample 50:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.5419196870425367e-06, 1.2225737009430304e-06, 0.9999972581863403]\n",
      "Sample 51:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [2.3661045815970283e-06, 1.4821523564023664e-06, 0.9999961853027344]\n",
      "Sample 52:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.750426054059062e-05, 9.367375923829968e-07, 0.9999815225601196]\n",
      "Sample 53:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.1533076076375437e-06, 8.350187954420107e-07, 0.9999979734420776]\n",
      "Sample 54:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [7.370231287495699e-07, 3.4484989441807556e-07, 0.999998927116394]\n",
      "Sample 55:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.8553305380919483e-06, 8.493010454913019e-07, 0.9999972581863403]\n",
      "Sample 56:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [3.841638317680918e-06, 3.32487638843304e-06, 0.999992847442627]\n",
      "Sample 57:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [4.508364781941054e-06, 2.1229477624729043e-06, 0.9999933242797852]\n",
      "Sample 58:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [2.258271933897049e-06, 5.636297260025458e-07, 0.9999971389770508]\n",
      "Sample 59:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.1423855994507903e-06, 4.3491434098541504e-07, 0.9999984502792358]\n",
      "Sample 60:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.6302755057040486e-06, 5.354710310712107e-07, 0.9999978542327881]\n",
      "Sample 61:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [4.307924882596126e-06, 3.5744042179430835e-06, 0.9999921321868896]\n",
      "Sample 62:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.4179103118294734e-06, 1.0814868574016145e-06, 0.9999974966049194]\n",
      "Sample 63:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [2.3661045815970283e-06, 1.4821523564023664e-06, 0.9999961853027344]\n",
      "Sample 64:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [2.5971733066398883e-06, 6.941235369595233e-06, 0.9999904632568359]\n",
      "Sample 65:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.015811927616596222, 8.833544598019216e-06, 0.9841791987419128]\n",
      "Sample 66:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [3.2705032936064526e-05, 8.521660492988303e-05, 0.9998821020126343]\n",
      "Sample 67:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.3610811038233805e-06, 9.862765182333533e-07, 0.999997615814209]\n",
      "Sample 68:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [7.553937848570058e-07, 4.260784862708533e-07, 0.9999988079071045]\n",
      "Sample 69:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.3853638172149658, 7.244912467285758e-06, 0.6146289110183716]\n",
      "Sample 70:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.2104836059734225e-05, 6.997918831075367e-07, 0.9999872446060181]\n",
      "Sample 71:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9994865655899048, 6.192936297111373e-08, 0.0005133001250214875]\n",
      "Sample 72:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [9.826724181039026e-07, 3.422608187975129e-07, 0.9999986886978149]\n",
      "Sample 73:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [3.3512294521642616e-06, 2.6057048216898693e-06, 0.9999940395355225]\n",
      "Sample 74:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [5.1347309636184946e-05, 0.02497325837612152, 0.9749753475189209]\n",
      "Sample 75:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [6.204676174093038e-05, 0.08868762105703354, 0.9112503528594971]\n",
      "Sample 76:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [3.006900988111738e-05, 3.814760930254124e-06, 0.9999661445617676]\n",
      "Sample 77:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.5938682028936455e-06, 0.9960630536079407, 0.003934376407414675]\n",
      "Sample 78:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [9.736309038999025e-07, 7.597338935738662e-07, 0.9999982118606567]\n",
      "Sample 79:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [2.7932999273616588e-06, 3.6513204122456955e-06, 0.9999935626983643]\n",
      "Sample 80:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [3.835331426671473e-06, 1.4636533705925103e-05, 0.9999815225601196]\n",
      "Sample 81:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.2676877076955861e-06, 1.0205544640484732e-06, 0.9999977350234985]\n",
      "Sample 82:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [4.007378265669104e-06, 3.0541471005562926e-06, 0.9999929666519165]\n",
      "Sample 83:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [7.490222060368978e-07, 3.9697954434814164e-07, 0.9999988079071045]\n",
      "Sample 84:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.987543873838149e-05, 7.712606020504609e-06, 0.9999724626541138]\n",
      "Sample 85:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [6.58862481941469e-05, 2.9232735414552735e-06, 0.9999312162399292]\n",
      "Sample 86:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [4.092634299013298e-06, 2.8257804842724e-06, 0.999993085861206]\n",
      "Sample 87:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.06608808040618896, 1.39067524287384e-05, 0.9338979721069336]\n",
      "Sample 88:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [6.840667197138828e-07, 2.8785055405933235e-07, 0.9999990463256836]\n",
      "Sample 89:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.001651030033826828, 2.2499632450490026e-06, 0.9983466863632202]\n",
      "Sample 90:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.1573334859349416e-06, 9.61124442255823e-07, 0.9999978542327881]\n",
      "Sample 91:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [6.4229236159008e-05, 0.0841556265950203, 0.9157801270484924]\n",
      "Sample 92:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [2.258271933897049e-06, 5.636297260025458e-07, 0.9999971389770508]\n",
      "Sample 93:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [2.0226186734362273e-06, 1.877388172033534e-06, 0.9999960660934448]\n",
      "Sample 94:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.1995250588370254e-06, 6.067209596949397e-07, 0.9999982118606567]\n",
      "Sample 95:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [6.103364739828976e-06, 5.7050815485126805e-06, 0.9999881982803345]\n",
      "Sample 96:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [3.980210294685094e-06, 1.2010136742901523e-06, 0.9999948740005493]\n",
      "Sample 97:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [6.700517474200751e-07, 3.0255131377998623e-07, 0.9999990463256836]\n",
      "Sample 98:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [2.3797113044565776e-06, 7.361499001490301e-07, 0.9999969005584717]\n",
      "Sample 99:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [6.132016778792604e-07, 2.540103025694407e-07, 0.9999991655349731]\n",
      "Sample 100:\n",
      "  - True Label: AHPND (2)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [9.090981620829552e-05, 1.1472037613202701e-06, 0.9999079704284668]\n",
      "Sample 101:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [3.436963993408426e-07, 0.9998918771743774, 0.00010780684533528984]\n",
      "Sample 102:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.9519138244893384e-09, 0.9999998807907104, 8.015755526002977e-08]\n",
      "Sample 103:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.673546856650887e-09, 1.0, 3.7408355524348735e-08]\n",
      "Sample 104:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [6.861899493060264e-09, 0.9999997615814209, 2.676866301953851e-07]\n",
      "Sample 105:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.1485162154656336e-08, 0.9999995231628418, 4.25378033241941e-07]\n",
      "Sample 106:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.0530557759125259e-08, 0.9999996423721313, 3.9907482118906046e-07]\n",
      "Sample 107:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.357628950543926e-10, 1.0, 1.1162955004806463e-08]\n",
      "Sample 108:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [9.042222970379044e-09, 0.9999996423721313, 3.9051343492246815e-07]\n",
      "Sample 109:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.7729505358564666e-09, 0.9999998807907104, 7.579863137152643e-08]\n",
      "Sample 110:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.655482722602187e-09, 0.9999998807907104, 6.863356816211308e-08]\n",
      "Sample 111:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.996752149352915e-10, 1.0, 1.278175343344401e-08]\n",
      "Sample 112:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.0947539763606073e-08, 0.9999995231628418, 5.343175644156872e-07]\n",
      "Sample 113:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.3269230159096423e-08, 0.9999994039535522, 5.85797408803046e-07]\n",
      "Sample 114:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.091051656971104e-08, 0.999990701675415, 9.13909843802685e-06]\n",
      "Sample 115:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [8.839922127634736e-10, 1.0, 1.5152153309827554e-08]\n",
      "Sample 116:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [4.306442846768732e-08, 0.9999951124191284, 4.830439593206393e-06]\n",
      "Sample 117:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [9.757691321610196e-10, 1.0, 1.7527058915334237e-08]\n",
      "Sample 118:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [9.042222970379044e-09, 0.9999996423721313, 3.9051343492246815e-07]\n",
      "Sample 119:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.3850367519552265e-09, 1.0, 2.7610035147063172e-08]\n",
      "Sample 120:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [4.031180278474267e-09, 0.9999998807907104, 1.2932302695389808e-07]\n",
      "Sample 121:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.360251800086587e-09, 1.0, 2.8231868398620463e-08]\n",
      "Sample 122:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.1234274782955254e-08, 0.9999990463256836, 9.106439620154561e-07]\n",
      "Sample 123:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [5.2629975932916295e-09, 0.9999997615814209, 1.982573678560584e-07]\n",
      "Sample 124:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [3.761804645563416e-09, 0.9999998807907104, 1.0924812698931419e-07]\n",
      "Sample 125:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.453738323637822e-09, 0.9999998807907104, 6.508962258067186e-08]\n",
      "Sample 126:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.0162527708246216e-08, 0.9999995231628418, 4.4077310690227023e-07]\n",
      "Sample 127:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.1694119006477877e-09, 1.0, 2.2735926918926452e-08]\n",
      "Sample 128:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [9.394446109922683e-09, 0.9999995231628418, 4.27967989935496e-07]\n",
      "Sample 129:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.4293815020494094e-09, 1.0, 2.8449223421489478e-08]\n",
      "Sample 130:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [6.960220844121068e-09, 0.9999997615814209, 2.7637548782877275e-07]\n",
      "Sample 131:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.2477799238297393e-08, 0.9999992847442627, 6.857088692413527e-07]\n",
      "Sample 132:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.3893751421866227e-09, 0.9999998807907104, 5.992404084054215e-08]\n",
      "Sample 133:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [8.326544786996237e-09, 0.9999996423721313, 3.963061772083165e-07]\n",
      "Sample 134:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [5.837041072709326e-09, 0.9999997615814209, 1.8089606612647913e-07]\n",
      "Sample 135:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.93461210887358e-09, 0.9999998807907104, 8.598748735266781e-08]\n",
      "Sample 136:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [0.00011959503899561241, 0.8868761658668518, 0.11300420761108398]\n",
      "Sample 137:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.4125454139701787e-09, 1.0, 2.9028599968228264e-08]\n",
      "Sample 138:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.859647155328275e-09, 0.9999996423721313, 3.0734500455764646e-07]\n",
      "Sample 139:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [3.436963993408426e-07, 0.9998918771743774, 0.00010780684533528984]\n",
      "Sample 140:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.9519138244893384e-09, 0.9999998807907104, 8.015755526002977e-08]\n",
      "Sample 141:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.144466299507485e-09, 1.0, 2.1522330584389238e-08]\n",
      "Sample 142:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.549485481206375e-09, 0.9999996423721313, 3.260457503984071e-07]\n",
      "Sample 143:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.6902086397152516e-09, 1.0, 3.7232034344469866e-08]\n",
      "Sample 144:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.549485481206375e-09, 0.9999996423721313, 3.260457503984071e-07]\n",
      "Sample 145:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.7256305273605221e-09, 1.0, 3.864090203364867e-08]\n",
      "Sample 146:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [8.039145349414412e-09, 0.9999996423721313, 3.5111219176542363e-07]\n",
      "Sample 147:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.2229273149699793e-09, 1.0, 2.3718767394598217e-08]\n",
      "Sample 148:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [9.109648146932159e-09, 0.9999995231628418, 4.331853915573447e-07]\n",
      "Sample 149:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [5.2629975932916295e-09, 0.9999997615814209, 1.982573678560584e-07]\n",
      "Sample 150:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [3.761804645563416e-09, 0.9999998807907104, 1.0924812698931419e-07]\n",
      "Sample 151:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.557204004105529e-09, 1.0, 5.083825627139049e-08]\n",
      "Sample 152:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.0023442520434855e-08, 0.9999995231628418, 4.779220148520835e-07]\n",
      "Sample 153:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [8.191027855986022e-10, 1.0, 1.3534667608894324e-08]\n",
      "Sample 154:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.0023442520434855e-08, 0.9999995231628418, 4.779220148520835e-07]\n",
      "Sample 155:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.5026089261738207e-09, 1.0, 3.1765502228608966e-08]\n",
      "Sample 156:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.0736141753397987e-08, 0.9999994039535522, 5.548249646381009e-07]\n",
      "Sample 157:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.056272180832707e-09, 1.0, 1.9182049726396144e-08]\n",
      "Sample 158:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.2646815150674229e-08, 0.9999992847442627, 7.582710850329022e-07]\n",
      "Sample 159:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.9240153914523717e-09, 1.0, 4.650743434808646e-08]\n",
      "Sample 160:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.4326125892694108e-05, 0.9897961020469666, 0.010189683176577091]\n",
      "Sample 161:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.996752149352915e-10, 1.0, 1.278175343344401e-08]\n",
      "Sample 162:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.0947539763606073e-08, 0.9999995231628418, 5.343175644156872e-07]\n",
      "Sample 163:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [3.891869493344302e-09, 0.9999998807907104, 1.2318359665641765e-07]\n",
      "Sample 164:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.638982143920998e-09, 0.9999998807907104, 6.822947540285895e-08]\n",
      "Sample 165:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [8.326544786996237e-09, 0.9999996423721313, 3.963061772083165e-07]\n",
      "Sample 166:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [5.837041072709326e-09, 0.9999997615814209, 1.8089606612647913e-07]\n",
      "Sample 167:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [4.2985663917249894e-09, 0.9999998807907104, 1.4998443020886043e-07]\n",
      "Sample 168:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.7497906412700104e-07, 0.9999885559082031, 1.1334027476550546e-05]\n",
      "Sample 169:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [8.326544786996237e-09, 0.9999996423721313, 3.963061772083165e-07]\n",
      "Sample 170:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [6.2445546511469274e-09, 0.9999997615814209, 2.2133896493414795e-07]\n",
      "Sample 171:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.6902086397152516e-09, 1.0, 3.7232034344469866e-08]\n",
      "Sample 172:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [6.270525876317379e-09, 0.9999997615814209, 2.4458390157633403e-07]\n",
      "Sample 173:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.4625751720842572e-09, 1.0, 3.1177791015579714e-08]\n",
      "Sample 174:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [6.960220844121068e-09, 0.9999997615814209, 2.7637548782877275e-07]\n",
      "Sample 175:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.381735836858411e-09, 1.0, 2.8260259909984597e-08]\n",
      "Sample 176:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.549485481206375e-09, 0.9999996423721313, 3.260457503984071e-07]\n",
      "Sample 177:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.096024271340923e-09, 1.0, 2.03221279804211e-08]\n",
      "Sample 178:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.0736141753397987e-08, 0.9999994039535522, 5.548249646381009e-07]\n",
      "Sample 179:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [8.26409329857114e-10, 1.0, 1.3357250416845545e-08]\n",
      "Sample 180:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [3.1863178762137068e-09, 0.9999998807907104, 8.551970864800751e-08]\n",
      "Sample 181:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.8002612733880596e-09, 1.0, 4.131828035269791e-08]\n",
      "Sample 182:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.549485481206375e-09, 0.9999996423721313, 3.260457503984071e-07]\n",
      "Sample 183:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.258260659170901e-10, 1.0, 1.0676784789609428e-08]\n",
      "Sample 184:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.202377361181789e-08, 0.9999908208847046, 9.017201591632329e-06]\n",
      "Sample 185:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [9.584322224753805e-10, 1.0, 1.6939468494570065e-08]\n",
      "Sample 186:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.5972020978315413e-08, 0.9999977350234985, 2.218968120359932e-06]\n",
      "Sample 187:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.783873545719189e-10, 1.0, 1.25243992954438e-08]\n",
      "Sample 188:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [4.929346886228814e-08, 0.9999938011169434, 6.166963430587202e-06]\n",
      "Sample 189:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.3531149534173892e-09, 1.0, 2.7235435240413608e-08]\n",
      "Sample 190:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.638185195446567e-09, 0.9999996423721313, 3.3187990311489557e-07]\n",
      "Sample 191:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.6902086397152516e-09, 1.0, 3.7232034344469866e-08]\n",
      "Sample 192:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.0023442520434855e-08, 0.9999995231628418, 4.779220148520835e-07]\n",
      "Sample 193:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [8.26409329857114e-10, 1.0, 1.3357250416845545e-08]\n",
      "Sample 194:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [3.1863178762137068e-09, 0.9999998807907104, 8.551970864800751e-08]\n",
      "Sample 195:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.0985775622529559e-09, 1.0, 2.0711684811658415e-08]\n",
      "Sample 196:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [6.263019436403283e-09, 0.9999997615814209, 2.45950360522329e-07]\n",
      "Sample 197:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [3.436963993408426e-07, 0.9998918771743774, 0.00010780684533528984]\n",
      "Sample 198:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.9519138244893384e-09, 0.9999998807907104, 8.015755526002977e-08]\n",
      "Sample 199:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [1.0985775622529559e-09, 1.0, 2.0711684811658415e-08]\n",
      "Sample 200:\n",
      "  - True Label: WSSV (1)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [7.859647155328275e-09, 0.9999996423721313, 3.0734500455764646e-07]\n",
      "Sample 201:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 5.080135201396274e-10, 3.2201126032305183e-07]\n",
      "Sample 202:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 4.083810511978925e-10, 2.1936175187420304e-07]\n",
      "Sample 203:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999992847442627, 1.0144547424317807e-09, 6.734189810231328e-07]\n",
      "Sample 204:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999984502792358, 1.745549482734532e-09, 1.4978189710745937e-06]\n",
      "Sample 205:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: WSSV (1)\n",
      "  - Probabilities: [2.6471593805865723e-09, 1.0, 5.5944244792272e-09]\n",
      "Sample 206:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 4.529246699913614e-10, 2.891003418881155e-07]\n",
      "Sample 207:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.4834181933108823e-10, 2.3517702629760606e-07]\n",
      "Sample 208:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.377127383823563e-10, 1.8571523696664372e-07]\n",
      "Sample 209:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999998807907104, 2.0552637369775084e-10, 1.7613025704577012e-07]\n",
      "Sample 210:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999992847442627, 1.9252182070772506e-09, 6.707020929752616e-07]\n",
      "Sample 211:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.0926602044492029e-05, 1.0009085826823139e-06, 0.9999880790710449]\n",
      "Sample 212:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 3.099802914352523e-10, 2.1524880366996513e-07]\n",
      "Sample 213:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.8881219638066113e-10, 2.2227011697850685e-07]\n",
      "Sample 214:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.1079672729573673e-10, 2.1863264976218488e-07]\n",
      "Sample 215:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.011005405336618423, 0.007879729382693768, 0.98111492395401]\n",
      "Sample 216:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 1.8234413978746034e-10, 1.818380326312763e-07]\n",
      "Sample 217:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999998807907104, 2.208376953527491e-10, 1.7862804213564232e-07]\n",
      "Sample 218:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.343266136684008e-10, 2.0462623240291578e-07]\n",
      "Sample 219:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999998807907104, 1.8011943325735302e-10, 1.670085367777574e-07]\n",
      "Sample 220:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999977350234985, 2.7048523421058235e-09, 2.2210535917110974e-06]\n",
      "Sample 221:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 3.6562236571668905e-10, 3.116340963060793e-07]\n",
      "Sample 222:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.160645690141294e-10, 2.062042057104918e-07]\n",
      "Sample 223:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999983310699463, 1.674690386366251e-09, 1.6176311419258127e-06]\n",
      "Sample 224:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999979734420776, 2.8912046090567856e-09, 1.9998497009510174e-06]\n",
      "Sample 225:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 3.957673633259162e-10, 4.0165627979149576e-07]\n",
      "Sample 226:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.2692098200494115e-10, 1.7955063924546266e-07]\n",
      "Sample 227:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 1.865515658616701e-10, 1.9084835400917655e-07]\n",
      "Sample 228:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 1.9892959501888186e-10, 2.0232329234204371e-07]\n",
      "Sample 229:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999998807907104, 2.1096206725967903e-10, 1.7009635655540478e-07]\n",
      "Sample 230:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 1.8421295044923625e-10, 2.018212796883745e-07]\n",
      "Sample 231:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 2.9117949718049374e-10, 3.644733794772037e-07]\n",
      "Sample 232:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9887616038322449, 3.0412777505262056e-06, 0.01123534794896841]\n",
      "Sample 233:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9989112615585327, 3.4529691674833884e-06, 0.001085255411453545]\n",
      "Sample 234:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999982118606567, 4.038585021959307e-09, 1.7667442762103747e-06]\n",
      "Sample 235:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 3.914894797230062e-10, 2.688363451852638e-07]\n",
      "Sample 236:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999836683273315, 1.6311528128198916e-08, 1.6295802197419107e-05]\n",
      "Sample 237:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 2.9881611074422665e-10, 3.1211442319545313e-07]\n",
      "Sample 238:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.999968409538269, 1.1113491638070627e-07, 3.152522913296707e-05]\n",
      "Sample 239:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 5.092727906053085e-10, 3.5412864463069127e-07]\n",
      "Sample 240:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.0016767808701843023, 7.442709465976804e-05, 0.998248815536499]\n",
      "Sample 241:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999983310699463, 5.103740985390459e-09, 1.6729353546907078e-06]\n",
      "Sample 242:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.3025785982608795, 4.882011853624135e-05, 0.6973726153373718]\n",
      "Sample 243:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 3.571786755252049e-10, 2.2609722805100319e-07]\n",
      "Sample 244:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999991655349731, 1.327116860849742e-09, 8.920772529563692e-07]\n",
      "Sample 245:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 1.934538224057647e-10, 1.8417283342841984e-07]\n",
      "Sample 246:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999991655349731, 7.91646470599261e-10, 8.131523827614728e-07]\n",
      "Sample 247:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 8.96666907390653e-10, 2.690725864340493e-07]\n",
      "Sample 248:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 3.4597028020222353e-10, 2.5608375153751695e-07]\n",
      "Sample 249:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 4.804897590915402e-10, 3.6076346532354364e-07]\n",
      "Sample 250:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999988079071045, 3.944145454681802e-09, 1.2512625744420802e-06]\n",
      "Sample 251:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 3.822083760596229e-10, 3.004593622790708e-07]\n",
      "Sample 252:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [1.0794265108415857e-05, 1.399510438204743e-06, 0.9999878406524658]\n",
      "Sample 253:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999992847442627, 6.37361663446967e-10, 6.562671615029103e-07]\n",
      "Sample 254:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 3.581419327769453e-10, 2.6905769345830777e-07]\n",
      "Sample 255:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 1.9775532600352363e-10, 1.874074087027111e-07]\n",
      "Sample 256:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 3.863466491171863e-10, 2.9612044727400644e-07]\n",
      "Sample 257:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999901056289673, 1.045416997413895e-08, 9.913022950058803e-06]\n",
      "Sample 258:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 5.058072294339411e-10, 2.6427926513861166e-07]\n",
      "Sample 259:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 1.9299609133049955e-10, 1.800156326225988e-07]\n",
      "Sample 260:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 3.810975424123342e-10, 3.9674256413491094e-07]\n",
      "Sample 261:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.4195084824540913e-10, 2.28396388024521e-07]\n",
      "Sample 262:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.061834000051021576, 2.14344727282878e-05, 0.9381445646286011]\n",
      "Sample 263:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 3.7232628091743436e-10, 2.7696992788150965e-07]\n",
      "Sample 264:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.3889953904010497e-10, 1.914871603503343e-07]\n",
      "Sample 265:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.370842688836916e-10, 2.2218259232431592e-07]\n",
      "Sample 266:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 3.08206321575355e-10, 2.623897046305501e-07]\n",
      "Sample 267:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.1085663770570306e-10, 2.0019551527639123e-07]\n",
      "Sample 268:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.999961256980896, 2.1758204127309e-08, 3.876480695907958e-05]\n",
      "Sample 269:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999978542327881, 2.138192956024909e-09, 2.1285322873154655e-06]\n",
      "Sample 270:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.302800311548836e-10, 2.1140886019566096e-07]\n",
      "Sample 271:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999994039535522, 5.188339757822291e-10, 5.478733555719373e-07]\n",
      "Sample 272:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.741374627301951e-10, 2.1495259261428146e-07]\n",
      "Sample 273:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999712705612183, 3.8265600466047545e-08, 2.8693511922028847e-05]\n",
      "Sample 274:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.00013499968918040395, 6.794778164476156e-05, 0.9997970461845398]\n",
      "Sample 275:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 3.3804745114274226e-10, 2.322326224657445e-07]\n",
      "Sample 276:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9832355976104736, 1.6281416037600138e-06, 0.016762837767601013]\n",
      "Sample 277:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9951733946800232, 3.182321108852193e-07, 0.004826299846172333]\n",
      "Sample 278:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9969422221183777, 6.842736638645874e-06, 0.0030508132185786963]\n",
      "Sample 279:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 4.4117620667805113e-10, 3.3985747904807795e-07]\n",
      "Sample 280:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999970197677612, 2.789642072897891e-09, 2.926478373410646e-06]\n",
      "Sample 281:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.40395592321363e-10, 2.3529301529379154e-07]\n",
      "Sample 282:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 3.02446317990146e-10, 3.02463206480752e-07]\n",
      "Sample 283:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 4.886179794105772e-10, 3.140788749078638e-07]\n",
      "Sample 284:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.1153015450359192e-10, 2.272769563660404e-07]\n",
      "Sample 285:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999994039535522, 5.761302546147817e-10, 6.020724754307594e-07]\n",
      "Sample 286:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 8.26792190267156e-10, 3.149105509692163e-07]\n",
      "Sample 287:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 3.612359578131219e-10, 3.297393504908541e-07]\n",
      "Sample 288:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 3.969989614827085e-10, 4.073959019024187e-07]\n",
      "Sample 289:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999995231628418, 1.3899452699916992e-09, 5.220884418122296e-07]\n",
      "Sample 290:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.465972981369191e-10, 2.4911264517868403e-07]\n",
      "Sample 291:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.2815134503861856e-10, 1.9176603416326543e-07]\n",
      "Sample 292:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.362185169690889e-10, 2.3841282370540284e-07]\n",
      "Sample 293:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999996423721313, 3.02446317990146e-10, 3.02463206480752e-07]\n",
      "Sample 294:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999992847442627, 8.545450458363746e-10, 6.867939532639866e-07]\n",
      "Sample 295:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.099721091441964e-10, 1.9124918537727353e-07]\n",
      "Sample 296:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.112688773925342e-10, 2.1773168157324108e-07]\n",
      "Sample 297:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.764615203432186e-10, 2.6688800858210016e-07]\n",
      "Sample 298:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.0272704348567316e-10, 2.0149145996128937e-07]\n",
      "Sample 299:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: AHPND (2)\n",
      "  - Probabilities: [0.00023147290630731732, 3.7472352687473176e-06, 0.9997647404670715]\n",
      "Sample 300:\n",
      "  - True Label: healthy (0)\n",
      "  - Predicted Class: healthy (0)\n",
      "  - Probabilities: [0.9999997615814209, 2.5398197434078895e-10, 2.3280072980469413e-07]\n"
     ]
    }
   ],
   "source": [
    "#Update testing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Custom Dataset for k-mer frequency data\n",
    "class KmerFrequencyDataset(Dataset):\n",
    "    def __init__(self, kmer_score_file):\n",
    "        self.data = pd.read_csv(kmer_score_file)\n",
    "        self.features = self.data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "        self.labels = self.data['status'].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True, dropout=0.5)\n",
    "        self.fc1 = nn.Linear(hidden_dim2, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "def load_data(kmer_score_file):\n",
    "    data = pd.read_csv(kmer_score_file)\n",
    "    features = data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "    true_labels = data['status'].values.astype(int)\n",
    "    return features, true_labels\n",
    "\n",
    "def load_trained_model(model_path, input_dim):\n",
    "    model = LSTMModel(input_dim=input_dim, embedding_dim=128, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_loader, label_mapping, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    all_probabilities = []\n",
    "    true_prediction_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device).float()\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            probabilities = outputs.cpu().numpy()\n",
    "            predictions = np.argmax(probabilities, axis=1)\n",
    "            \n",
    "            # Store probabilities for true predictions\n",
    "            for i, pred in enumerate(predictions):\n",
    "                if pred == y_batch[i].item():\n",
    "                    true_prediction_probs.append(probabilities[i][pred])\n",
    "\n",
    "            all_probabilities.extend(probabilities)\n",
    "            all_predictions.extend(predictions)\n",
    "            all_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    return all_predictions, all_true_labels, all_probabilities, true_prediction_probs\n",
    "\n",
    "# Load test dataset\n",
    "kmer_score_file = '/home/user/torch_shrimp/until-tools/mod/k-mer/dataset/test5101.csv'\n",
    "lstm_model = '/home/user/torch_shrimp/until-tools/mod/k-mer/es/batch1/best_model.pth'\n",
    "features, true_labels = load_data(kmer_score_file)\n",
    "\n",
    "# Create DataLoader for test set\n",
    "test_samples = torch.tensor(features)\n",
    "test_labels = torch.tensor(true_labels)\n",
    "batch_size = 32\n",
    "test_dataset = torch.utils.data.TensorDataset(test_samples, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping = {0: 'healthy', 1: 'WSSV', 2: 'AHPND'}\n",
    "\n",
    "# Load the trained model\n",
    "model = load_trained_model(lstm_model, input_dim=features.shape[1])\n",
    "\n",
    "# Test the model and get predictions\n",
    "predictions, true_labels, probabilities, true_prediction_probs = test_model(model, test_loader, label_mapping, device='cpu')\n",
    "\n",
    "# Compute statistics\n",
    "num_true_predictions = sum([1 for i in range(len(predictions)) if predictions[i] == true_labels[i]])\n",
    "num_false_predictions = len(predictions) - num_true_predictions\n",
    "avg_true_prediction_prob = np.mean(true_prediction_probs) if true_prediction_probs else 0.0\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Average True Prediction Probability: {avg_true_prediction_prob:.4f}\")\n",
    "print(f\"Number of True Predictions: {num_true_predictions} ({(num_true_predictions / len(predictions)) * 100:.2f}%)\")\n",
    "print(f\"Number of False Predictions: {num_false_predictions} ({(num_false_predictions / len(predictions)) * 100:.2f}%)\")\n",
    "print(\"\\nPredictions for test samples:\")\n",
    "\n",
    "# Print individual predictions\n",
    "for i, pred in enumerate(predictions):\n",
    "    true_label = true_labels[i]\n",
    "    predicted_label = label_mapping[pred]\n",
    "    true_label_name = label_mapping[true_label]\n",
    "\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  - True Label: {true_label_name} ({true_label})\")\n",
    "    print(f\"  - Predicted Class: {predicted_label} ({pred})\")\n",
    "    print(f\"  - Probabilities: {probabilities[i].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62625172-9aea-4211-aa32-8d62fb2ec1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New testing with row indication\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Custom Dataset for k-mer frequency data\n",
    "class KmerFrequencyDataset(Dataset):\n",
    "    def __init__(self, kmer_score_file):\n",
    "        # Load scoring file\n",
    "        self.data = pd.read_csv(kmer_score_file)\n",
    "\n",
    "        # Extract features and labels\n",
    "        self.features = self.data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "        self.labels = self.data['status'].values.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "\n",
    "# Define LSTM model with embedding layer\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True, dropout=0.5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim2, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_dim) -> (batch_size, embedding_dim)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension for LSTM\n",
    "\n",
    "        # LSTM layers\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "\n",
    "        # Extract the last output of the LSTM\n",
    "        x = x[:, -1, :]  # Use the last hidden state\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)  # Output probabilities\n",
    "\n",
    "def load_data(kmer_score_file):\n",
    "    data = pd.read_csv(kmer_score_file)\n",
    "    features = data.drop(['status'], axis=1).values.astype(np.float32)\n",
    "    true_labels = data['status'].values.astype(int)\n",
    "    return data, features, true_labels\n",
    "\n",
    "def load_trained_model(model_path, input_dim):\n",
    "    # Define the model architecture\n",
    "    model = LSTMModel(input_dim=input_dim, embedding_dim=128, hidden_dim1=64, hidden_dim2=32, dense_units=64, num_classes=3)\n",
    "\n",
    "    # Load the saved model weights\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_loader, label_mapping, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device).float()\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(X_batch)\n",
    "            probabilities = outputs.cpu().numpy()\n",
    "            predictions = np.argmax(probabilities, axis=1)\n",
    "\n",
    "            all_probabilities.extend(probabilities)\n",
    "            all_predictions.extend(predictions)\n",
    "            all_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    return all_predictions, all_true_labels, all_probabilities\n",
    "\n",
    "# Load test dataset\n",
    "kmer_score_file = '/home/user/torch_shrimp/until-tools/mod/k-mer/test5101.csv'  # Update this with your file path\n",
    "lstm_model = '/home/user/torch_shrimp/until-tools/mod/k-mer/file_tune1.pth'\n",
    "raw_data, features, true_labels = load_data(kmer_score_file)\n",
    "\n",
    "# Create DataLoader for test set\n",
    "test_samples = torch.tensor(features)\n",
    "test_labels = torch.tensor(true_labels)\n",
    "batch_size = 32\n",
    "test_dataset = torch.utils.data.TensorDataset(test_samples, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define label mapping (ensure this matches your labels)\n",
    "label_mapping = {0: 'healthy', 1: 'WSSV', 2: 'ANPHD'}  # Example mapping\n",
    "\n",
    "# Load the trained model\n",
    "model = load_trained_model(lstm_model, input_dim=features.shape[1])\n",
    "\n",
    "# Test the model and get predictions\n",
    "predictions, true_labels, probabilities = test_model(model, test_loader, label_mapping, device='cpu')\n",
    "\n",
    "# Print predictions for each test sample\n",
    "for i, pred in enumerate(predictions):\n",
    "    true_label = true_labels[i]\n",
    "    predicted_label = label_mapping[pred]  # Map predicted class to label\n",
    "    true_label_name = label_mapping[true_label]  # Map true class to label\n",
    "    row_data = raw_data.iloc[i].to_dict()  # Extract the corresponding row from the dataset\n",
    "\n",
    "    print(f\"Sample {i + 1} (Row {i}):\")\n",
    "    print(f\"  - Row Data: {row_data}\")\n",
    "    print(f\"  - True Label: {true_label_name} ({true_label})\")\n",
    "    print(f\"  - Predicted Class: {predicted_label} ({pred})\")\n",
    "    print(f\"  - Probabilities: {probabilities[i].tolist()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59b8b1-9114-417a-8ed0-08801c1ad74b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shrimp_kernel",
   "language": "python",
   "name": "shrimp_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
